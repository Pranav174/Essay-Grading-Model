{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./asap-aes/training_set_rel3.tsv', sep='\\t', encoding = \"latin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav_pro/SchoolSystem/.venv/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "essay_id          12701\n",
       "essay_set         12701\n",
       "essay             12701\n",
       "rater1_domain1    12701\n",
       "rater2_domain1    12701\n",
       "rater3_domain1      128\n",
       "domain1_score     12701\n",
       "rater1_domain2     1799\n",
       "rater2_domain2     1799\n",
       "domain2_score      1799\n",
       "rater1_trait1      2055\n",
       "rater1_trait2      2055\n",
       "rater1_trait3      2055\n",
       "rater1_trait4      2055\n",
       "rater1_trait5       722\n",
       "rater1_trait6       722\n",
       "rater2_trait1      2055\n",
       "rater2_trait2      2055\n",
       "rater2_trait3      2055\n",
       "rater2_trait4      2055\n",
       "rater2_trait5       722\n",
       "rater2_trait6       722\n",
       "rater3_trait1       128\n",
       "rater3_trait2       128\n",
       "rater3_trait3       128\n",
       "rater3_trait4       128\n",
       "rater3_trait5       128\n",
       "rater3_trait6       128\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['illegible']=data.essay.str.contains('(\\?\\?\\?|illegible|not legible)')\n",
    "data = data[data.illegible==False]\n",
    "data = data.drop('illegible', 1)\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['essay'] = data['essay'].apply(lambda x: re.sub(' +', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>rater1_trait1</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12701.000000</td>\n",
       "      <td>12701.000000</td>\n",
       "      <td>12701.000000</td>\n",
       "      <td>12701.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>12701.000000</td>\n",
       "      <td>1799.000000</td>\n",
       "      <td>1799.000000</td>\n",
       "      <td>1799.000000</td>\n",
       "      <td>2055.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2055.000000</td>\n",
       "      <td>2055.000000</td>\n",
       "      <td>722.000000</td>\n",
       "      <td>722.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10146.245020</td>\n",
       "      <td>4.129832</td>\n",
       "      <td>4.057791</td>\n",
       "      <td>4.068341</td>\n",
       "      <td>37.828125</td>\n",
       "      <td>6.635226</td>\n",
       "      <td>3.334074</td>\n",
       "      <td>3.330739</td>\n",
       "      <td>3.334074</td>\n",
       "      <td>2.504136</td>\n",
       "      <td>...</td>\n",
       "      <td>2.708029</td>\n",
       "      <td>2.773723</td>\n",
       "      <td>3.778393</td>\n",
       "      <td>3.590028</td>\n",
       "      <td>3.945312</td>\n",
       "      <td>3.890625</td>\n",
       "      <td>4.078125</td>\n",
       "      <td>3.992188</td>\n",
       "      <td>3.843750</td>\n",
       "      <td>3.617188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6266.038381</td>\n",
       "      <td>2.123600</td>\n",
       "      <td>4.205748</td>\n",
       "      <td>4.259040</td>\n",
       "      <td>5.240829</td>\n",
       "      <td>8.945280</td>\n",
       "      <td>0.729263</td>\n",
       "      <td>0.726967</td>\n",
       "      <td>0.729263</td>\n",
       "      <td>1.229647</td>\n",
       "      <td>...</td>\n",
       "      <td>1.168146</td>\n",
       "      <td>1.061814</td>\n",
       "      <td>0.689271</td>\n",
       "      <td>0.693389</td>\n",
       "      <td>0.643668</td>\n",
       "      <td>0.630390</td>\n",
       "      <td>0.622535</td>\n",
       "      <td>0.509687</td>\n",
       "      <td>0.538845</td>\n",
       "      <td>0.603417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4372.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9941.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15513.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>21633.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           essay_id     essay_set  rater1_domain1  rater2_domain1  \\\n",
       "count  12701.000000  12701.000000    12701.000000    12701.000000   \n",
       "mean   10146.245020      4.129832        4.057791        4.068341   \n",
       "std     6266.038381      2.123600        4.205748        4.259040   \n",
       "min        1.000000      1.000000        0.000000        0.000000   \n",
       "25%     4372.000000      2.000000        2.000000        2.000000   \n",
       "50%     9941.000000      4.000000        3.000000        3.000000   \n",
       "75%    15513.000000      6.000000        4.000000        4.000000   \n",
       "max    21633.000000      8.000000       30.000000       30.000000   \n",
       "\n",
       "       rater3_domain1  domain1_score  rater1_domain2  rater2_domain2  \\\n",
       "count      128.000000   12701.000000     1799.000000     1799.000000   \n",
       "mean        37.828125       6.635226        3.334074        3.330739   \n",
       "std          5.240829       8.945280        0.729263        0.726967   \n",
       "min         20.000000       0.000000        1.000000        1.000000   \n",
       "25%         36.000000       2.000000        3.000000        3.000000   \n",
       "50%         40.000000       3.000000        3.000000        3.000000   \n",
       "75%         40.000000       8.000000        4.000000        4.000000   \n",
       "max         50.000000      60.000000        4.000000        4.000000   \n",
       "\n",
       "       domain2_score  rater1_trait1  ...  rater2_trait3  rater2_trait4  \\\n",
       "count    1799.000000    2055.000000  ...    2055.000000    2055.000000   \n",
       "mean        3.334074       2.504136  ...       2.708029       2.773723   \n",
       "std         0.729263       1.229647  ...       1.168146       1.061814   \n",
       "min         1.000000       0.000000  ...       0.000000       0.000000   \n",
       "25%         3.000000       2.000000  ...       2.000000       2.000000   \n",
       "50%         3.000000       2.000000  ...       2.000000       3.000000   \n",
       "75%         4.000000       3.000000  ...       4.000000       4.000000   \n",
       "max         4.000000       6.000000  ...       6.000000       6.000000   \n",
       "\n",
       "       rater2_trait5  rater2_trait6  rater3_trait1  rater3_trait2  \\\n",
       "count     722.000000     722.000000     128.000000     128.000000   \n",
       "mean        3.778393       3.590028       3.945312       3.890625   \n",
       "std         0.689271       0.693389       0.643668       0.630390   \n",
       "min         1.000000       1.000000       2.000000       2.000000   \n",
       "25%         3.000000       3.000000       4.000000       4.000000   \n",
       "50%         4.000000       4.000000       4.000000       4.000000   \n",
       "75%         4.000000       4.000000       4.000000       4.000000   \n",
       "max         6.000000       6.000000       6.000000       6.000000   \n",
       "\n",
       "       rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n",
       "count     128.000000     128.000000     128.000000     128.000000  \n",
       "mean        4.078125       3.992188       3.843750       3.617188  \n",
       "std         0.622535       0.509687       0.538845       0.603417  \n",
       "min         2.000000       3.000000       2.000000       2.000000  \n",
       "25%         4.000000       4.000000       4.000000       3.000000  \n",
       "50%         4.000000       4.000000       4.000000       4.000000  \n",
       "75%         4.000000       4.000000       4.000000       4.000000  \n",
       "max         6.000000       6.000000       5.000000       5.000000  \n",
       "\n",
       "[8 rows x 27 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in tqdm(f):\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "221it [00:00, 2209.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400001it [00:36, 10845.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. 400001  words loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "glove = loadGloveModel('glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, verbose =  True):\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator \n",
    "\n",
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(essay, replaceCommon, removeSW=False):\n",
    "    if removeSW:\n",
    "        for punct in \"/-'\":\n",
    "            essay = essay.replace(punct, ' ')\n",
    "        for punct in '&':\n",
    "            essay = essay.replace(punct, f' {punct} ')\n",
    "        for punct in '?!.,\"#$%\\'()*+-/:;<=>[\\\\]^_`{|}~' + '“”’':\n",
    "            essay = essay.replace(punct, '')\n",
    "    essay = essay.lower()\n",
    "    essay = re.sub(\"\\x92\",\"'\",essay)\n",
    "    essay = re.sub(\"\\x96\",\"-\",essay)\n",
    "    essay = re.sub(\"\\x93\",'\"',essay)\n",
    "    essay = re.sub(\"\\x94\",'\"',essay)\n",
    "    essay = re.sub(\"n't\",\" not\",essay)\n",
    "    essay = re.sub(\"'re\",\" are\",essay)\n",
    "    essay = re.sub(\"it's\",\"it is\",essay)\n",
    "    essay = re.sub(\"'ve'\",\" have\",essay)\n",
    "    for punct in ',.&?)!\"(:\\'':\n",
    "        essay = essay.replace(punct, f' {punct} ')\n",
    "    #replace ner to common example\n",
    "    if replaceCommon:\n",
    "        essay = re.sub(\"@person[^ ]*\",\"john\",essay)\n",
    "        essay = re.sub(\"@month[^ ]*\",\"january\",essay)\n",
    "        essay = re.sub(\"@location[^ ]*\",\"italy\",essay)\n",
    "        essay = re.sub(\"@num[^ ]*\",\"1\",essay)\n",
    "        essay = re.sub(\"@organization[^ ]*\",\"google\",essay)\n",
    "        essay = re.sub(\"@date[^ ]*\",\"3rd may\",essay)\n",
    "        essay = re.sub(\"@percent[^ ]*\",\"100 %\",essay)\n",
    "        essay = re.sub(\"@money[^ ]*\",\"$ 1\",essay)\n",
    "        essay = re.sub(\"@time[^ ]*\",\"1am\",essay)\n",
    "        essay = re.sub(\"@dr[^ ]*\",\"dr john\",essay)\n",
    "        essay = re.sub(\"@caps[^ ]*\",\"peter\",essay)\n",
    "    return essay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeEssayArr(replaceCommon, removeSW):\n",
    "    sentences = []\n",
    "    for essay in data.essay:\n",
    "        essay = clean(essay,replaceCommon=replaceCommon, removeSW=removeSW)\n",
    "        essay = essay.split()\n",
    "        essay = list(filter(None, essay))\n",
    "        sentences.append(essay)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12701/12701 [00:00<00:00, 20776.53it/s]\n",
      "100%|██████████| 39911/39911 [00:00<00:00, 782903.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 53.51% of vocab\n",
      "Found embeddings for  98.90% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('greatful', 281),\n",
       " ('kurmaskie', 267),\n",
       " ('lüsted', 263),\n",
       " ('shelfs', 188),\n",
       " ('shouldnt', 151),\n",
       " ('thousand-foot', 142),\n",
       " ('benifit', 134),\n",
       " ('diffrent', 133),\n",
       " ('minfong', 130),\n",
       " ('flamable', 124),\n",
       " ('libary', 114),\n",
       " ('auther', 109),\n",
       " ('cordination', 107),\n",
       " ('somthing', 105),\n",
       " ('librarys', 102),\n",
       " ('obsticles', 100),\n",
       " ('inapropriate', 83),\n",
       " ('innapropriate', 83),\n",
       " ('offened', 76),\n",
       " ('childern', 75),\n",
       " ('obsticle', 73),\n",
       " ('familys', 65),\n",
       " ('freinds', 62),\n",
       " ('differnt', 61),\n",
       " ('¶', 59),\n",
       " ('certian', 55),\n",
       " ('exersize', 52),\n",
       " ('comunicate', 52),\n",
       " ('useing', 52),\n",
       " ('librarie', 51),\n",
       " ('reson', 51),\n",
       " ('magizines', 49),\n",
       " ('computors', 49),\n",
       " ('excersise', 48),\n",
       " ('inappropiate', 47),\n",
       " ('benifits', 47),\n",
       " ('â\\x80\\x9d', 46),\n",
       " ('childeren', 46),\n",
       " ('chating', 46),\n",
       " ('everyones', 46),\n",
       " ('memior', 45),\n",
       " ('offencive', 44),\n",
       " ('completly', 44),\n",
       " ('excersize', 44),\n",
       " ('exersise', 44),\n",
       " ('greatfulness', 43),\n",
       " ('buisnesses', 43),\n",
       " ('gratefull', 41),\n",
       " ('appropiate', 41),\n",
       " ('insted', 41),\n",
       " ('libaries', 40),\n",
       " ('definetly', 40),\n",
       " ('oppinion', 40),\n",
       " ('chrome-nickel', 39),\n",
       " ('possitive', 39),\n",
       " ('drigibles', 37),\n",
       " ('haveing', 37),\n",
       " ('offinsive', 35),\n",
       " ('innappropriate', 35),\n",
       " ('â\\x80\\x9cpeter', 34),\n",
       " ('sacrafice', 34),\n",
       " ('certin', 34),\n",
       " ('computor', 34),\n",
       " ('negitive', 34),\n",
       " ('usefull', 34),\n",
       " ('greatfull', 33),\n",
       " ('becaus', 33),\n",
       " ('wheather', 33),\n",
       " ('cencorship', 32),\n",
       " ('magizine', 32),\n",
       " ('intrest', 32),\n",
       " ('writting', 32),\n",
       " ('werent', 31),\n",
       " ('\\x91', 30),\n",
       " ('tryed', 30),\n",
       " ('helpfull', 30),\n",
       " ('libray', 29),\n",
       " ('relized', 29),\n",
       " ('personaly', 28),\n",
       " ('pacient', 27),\n",
       " ('thankfull', 27),\n",
       " ('makeing', 27),\n",
       " ('relize', 27),\n",
       " ('excersising', 27),\n",
       " ('allways', 27),\n",
       " ('resons', 27),\n",
       " ('peice', 26),\n",
       " ('definatly', 26),\n",
       " ('gratefullness', 25),\n",
       " ('probly', 25),\n",
       " ('excercising', 25),\n",
       " ('unpatient', 24),\n",
       " ('derigibles', 24),\n",
       " ('enviornment', 24),\n",
       " ('veiw', 24),\n",
       " ('dosent', 24),\n",
       " ('wating', 24),\n",
       " ('selfs', 24),\n",
       " ('obsticals', 23),\n",
       " ('thoes', 23),\n",
       " ('imformation', 23),\n",
       " ('posotive', 23),\n",
       " ('comunication', 23),\n",
       " ('opion', 23),\n",
       " ('finnaly', 23),\n",
       " ('themselfs', 23),\n",
       " ('cencored', 22),\n",
       " ('tooken', 22),\n",
       " ('abhor-', 22),\n",
       " ('enviorment', 22),\n",
       " ('it;', 22),\n",
       " ('socity', 22),\n",
       " ('enternet', 22),\n",
       " ('bestfriend', 22),\n",
       " ('basicly', 22),\n",
       " ('immagrants', 21),\n",
       " ('veiws', 21),\n",
       " ('easyer', 21),\n",
       " ('activitys', 21),\n",
       " ('exersice', 21),\n",
       " ('conclution', 21),\n",
       " ('tarlike', 20),\n",
       " ('creats', 20),\n",
       " ('takeing', 20),\n",
       " ('becase', 20),\n",
       " ('beggining', 20),\n",
       " ('exersizing', 20),\n",
       " ('buissness', 20),\n",
       " ('drigible', 19),\n",
       " ('parants', 19),\n",
       " ('becouse', 19),\n",
       " ('shoping', 19),\n",
       " ('rember', 19),\n",
       " ('beacause', 19),\n",
       " ('controll', 19),\n",
       " ('authour', 18),\n",
       " ('abhor-then', 18),\n",
       " ('shevles', 18),\n",
       " ('shelf-', 18),\n",
       " ('whent', 18),\n",
       " ('dieing', 18),\n",
       " ('comptuer', 18),\n",
       " ('exercis', 18),\n",
       " ('probaly', 18),\n",
       " ('likly', 18),\n",
       " ('dirigables', 17),\n",
       " ('imigrants', 17),\n",
       " ('exerpt', 17),\n",
       " ('shelf-that', 17),\n",
       " ('writen', 17),\n",
       " ('absolutly', 17),\n",
       " ('agian', 17),\n",
       " ('typeing', 17),\n",
       " ('exercize', 17),\n",
       " ('remeber', 17),\n",
       " ('knoledge', 17),\n",
       " ('alow', 17),\n",
       " ('labtop', 17),\n",
       " ('didn`t', 16),\n",
       " ('censore', 16),\n",
       " ('censorships', 16),\n",
       " ('cenorship', 16),\n",
       " ('magzines', 16),\n",
       " ('catagories', 16),\n",
       " ('affensive', 16),\n",
       " ('inapropiate', 16),\n",
       " ('whant', 16),\n",
       " ('anothe', 16),\n",
       " ('intresting', 16),\n",
       " ('comming', 16),\n",
       " ('comuter', 16),\n",
       " ('awsome', 16),\n",
       " ('benificial', 16),\n",
       " ('wated', 15),\n",
       " ('happieness', 15),\n",
       " ('rawedia', 15),\n",
       " ('careing', 15),\n",
       " ('water-depleting', 15),\n",
       " ('me;', 15),\n",
       " ('moive', 15),\n",
       " ('oppinions', 15),\n",
       " ('rediculous', 15),\n",
       " ('alittle', 15),\n",
       " ('libraies', 15),\n",
       " ('apropriate', 15),\n",
       " ('coordiantion', 15),\n",
       " ('probally', 15),\n",
       " ('excersizing', 15),\n",
       " ('avalible', 15),\n",
       " ('vaction', 15),\n",
       " ('oppertunity', 15),\n",
       " ('oovoo', 15),\n",
       " ('studing', 15),\n",
       " ('peolpe', 15),\n",
       " ('dirigble', 14),\n",
       " ('flamible', 14),\n",
       " ('pharagraph', 14),\n",
       " ('cycleist', 14),\n",
       " ('liturature', 14),\n",
       " ('disapointed', 14),\n",
       " ('offesive', 14),\n",
       " ('libraires', 14),\n",
       " ('beging', 14),\n",
       " ('speach', 14),\n",
       " ('peopel', 14),\n",
       " ('<100', 14),\n",
       " ('internent', 14),\n",
       " ('adicted', 14),\n",
       " ('flameable', 13),\n",
       " ('obstical', 13),\n",
       " ('building;', 13),\n",
       " ('grabed', 13),\n",
       " ('oppion', 13),\n",
       " ('decied', 13),\n",
       " ('alowed', 13),\n",
       " ('excersing', 13),\n",
       " ('everone', 13),\n",
       " ('beable', 13),\n",
       " ('extreamly', 13),\n",
       " ('obeast', 13),\n",
       " ('now-a-days', 13),\n",
       " ('wouldent', 13),\n",
       " ('couldent', 13),\n",
       " ('cordanation', 13),\n",
       " ('benefitial', 13),\n",
       " ('friends/family', 13),\n",
       " ('unfortunatly', 13),\n",
       " ('a+', 13),\n",
       " ('dirigbles', 12),\n",
       " ('loveing', 12),\n",
       " ('discribes', 12),\n",
       " ('cyclsit', 12),\n",
       " ('discribed', 12),\n",
       " ('life;', 12),\n",
       " ('offeneded', 12),\n",
       " ('censory', 12),\n",
       " ('apon', 12),\n",
       " ('moives', 12),\n",
       " ('libaray', 12),\n",
       " ('violance', 12),\n",
       " ('neiborhood', 12),\n",
       " ('intellegent', 12),\n",
       " ('whould', 12),\n",
       " ('oppions', 12),\n",
       " ('somtimes', 12),\n",
       " ('oppurtunity', 12),\n",
       " ('shuld', 12),\n",
       " ('world;', 12),\n",
       " ('excrise', 12),\n",
       " ('knowlage', 12),\n",
       " ('convient', 12),\n",
       " ('shold', 12),\n",
       " ('maby', 12),\n",
       " ('comuters', 12),\n",
       " ('vidio', 12),\n",
       " ('reaserch', 12),\n",
       " ('pacience', 11),\n",
       " ('dirigable', 11),\n",
       " ('obsticales', 11),\n",
       " ('[his]', 11),\n",
       " ('cyclest', 11),\n",
       " ('intitled', 11),\n",
       " ('offensive;', 11),\n",
       " ('beleave', 11),\n",
       " ('relizes', 11),\n",
       " ('diffent', 11),\n",
       " ('ammount', 11),\n",
       " ('people;', 11),\n",
       " ('actualy', 11),\n",
       " ('habbits', 11),\n",
       " ('probley', 11),\n",
       " ('usally', 11),\n",
       " ('anywere', 11),\n",
       " ('@state1', 11),\n",
       " ('theese', 11),\n",
       " ('alows', 11),\n",
       " ('familly', 11),\n",
       " ('x-box', 11),\n",
       " ('inconclusion', 11),\n",
       " ('example;', 11),\n",
       " ('thoughs', 11),\n",
       " ('benifiting', 11),\n",
       " ('arguements', 11),\n",
       " ('frends', 11),\n",
       " ('artical', 11),\n",
       " ('obease', 11),\n",
       " ('things;', 11),\n",
       " ('patiences', 10),\n",
       " ('greatfullness', 10),\n",
       " ('gratful', 10),\n",
       " ('thankfullness', 10),\n",
       " ('memorys', 10),\n",
       " ('desserted', 10),\n",
       " ('ahead;', 10),\n",
       " ('it`s', 10),\n",
       " ('romoved', 10),\n",
       " ('abhor--then', 10),\n",
       " ('eveyone', 10),\n",
       " ('sensored', 10),\n",
       " ('computar', 10),\n",
       " ('library;', 10),\n",
       " ('suppost', 10),\n",
       " ('waching', 10),\n",
       " ('way;', 10),\n",
       " ('<1', 10),\n",
       " ('experince', 10),\n",
       " ('anyones', 10),\n",
       " ('succed', 10),\n",
       " ('supose', 10),\n",
       " ('convinient', 10),\n",
       " ('innapropiate', 10),\n",
       " ('beleve', 10),\n",
       " ('pritty', 10),\n",
       " ('opend', 10),\n",
       " ('is;', 10),\n",
       " ('her;', 10),\n",
       " ('benfit', 10),\n",
       " ('unappropriate', 10),\n",
       " ('diferent', 10),\n",
       " ('inportant', 10),\n",
       " ('w/', 10),\n",
       " ('frinds', 10),\n",
       " ('wernt', 10),\n",
       " ('exsample', 10),\n",
       " ('dident', 10),\n",
       " ('videochat', 10),\n",
       " ('compters', 10),\n",
       " ('easly', 10),\n",
       " ('handeye', 10),\n",
       " ('soceity', 10),\n",
       " ('adays', 10),\n",
       " ('laughter;', 9),\n",
       " ('obstacals', 9),\n",
       " ('dirgibles', 9),\n",
       " ('densly', 9),\n",
       " ('ontop', 9),\n",
       " ('derigible', 9),\n",
       " ('went-and', 9),\n",
       " ('blond-brick', 9),\n",
       " ('hapiness', 9),\n",
       " ('vowe', 9),\n",
       " ('don`t', 9),\n",
       " ('riadin', 9),\n",
       " ('lilbrary', 9),\n",
       " ('liberary', 9),\n",
       " ('unexceptable', 9),\n",
       " ('labraries', 9),\n",
       " ('niether', 9),\n",
       " ('sombody', 9),\n",
       " ('non-offensive', 9),\n",
       " ('alchol', 9),\n",
       " ('inappropraite', 9),\n",
       " ('vulger', 9),\n",
       " ('deside', 9),\n",
       " ('shouldent', 9),\n",
       " ('comeing', 9),\n",
       " ('awile', 9),\n",
       " ('sincerly', 9),\n",
       " ('litature', 9),\n",
       " ('compiuters', 9),\n",
       " ('peices', 9),\n",
       " ('accessable', 9),\n",
       " ('learing', 9),\n",
       " ('relitives', 9),\n",
       " ('expierence', 9),\n",
       " ('perfer', 9),\n",
       " ('lieing', 9),\n",
       " ('truble', 9),\n",
       " ('obeasity', 9),\n",
       " ('belifs', 9),\n",
       " ('arive', 9),\n",
       " ('#1', 9),\n",
       " ('commputer', 9),\n",
       " ('whay', 9),\n",
       " ('pepole', 9),\n",
       " ('voilence', 9),\n",
       " ('tuch', 9),\n",
       " ('wasent', 9),\n",
       " ('vertual', 9),\n",
       " ('faimly', 9),\n",
       " ('posative', 9),\n",
       " ('socialy', 9),\n",
       " ('usualy', 9),\n",
       " ('exersising', 9),\n",
       " ('weither', 9),\n",
       " ('concideration', 9),\n",
       " ('benifical', 9),\n",
       " ('excersice', 9),\n",
       " ('cheak', 9),\n",
       " ('excerise', 9),\n",
       " ('beileve', 9),\n",
       " ('habbit', 9),\n",
       " ('troble', 9),\n",
       " ('gt;', 8),\n",
       " ('pation', 8),\n",
       " ('paitent', 8),\n",
       " ('stepped-back', 8),\n",
       " ('clavan', 8),\n",
       " ('diribles', 8),\n",
       " ('hellium', 8),\n",
       " ('happines', 8),\n",
       " ('¶1', 8),\n",
       " ('joyfull', 8),\n",
       " ('graditude', 8),\n",
       " ('laught', 8),\n",
       " ('dehidrated', 8),\n",
       " ('patien', 8),\n",
       " ('libarie', 8),\n",
       " ('becaues', 8),\n",
       " ('i-pod', 8),\n",
       " ('shelvs', 8),\n",
       " ('exspecially', 8),\n",
       " ('books;', 8),\n",
       " ('boreing', 8),\n",
       " ('durring', 8),\n",
       " ('offensice', 8),\n",
       " ('ofensive', 8),\n",
       " ('authers', 8),\n",
       " ('concidered', 8),\n",
       " ('inapporiate', 8),\n",
       " ('elementry', 8),\n",
       " ('matterial', 8),\n",
       " ('cencor', 8),\n",
       " ('beleifs', 8),\n",
       " ('approprite', 8),\n",
       " ('magzine', 8),\n",
       " ('opinons', 8),\n",
       " ('shelf--that', 8),\n",
       " ('chilren', 8),\n",
       " ('finly', 8),\n",
       " ('desided', 8),\n",
       " ('computure', 8),\n",
       " ('charater', 8),\n",
       " ('time;', 8),\n",
       " ('them;', 8),\n",
       " ('\\x91peter', 8),\n",
       " ('intill', 8),\n",
       " ('finnally', 8),\n",
       " ('incase', 8),\n",
       " ('ofcourse', 8),\n",
       " ('deffinetly', 8),\n",
       " ('rideing', 8),\n",
       " ('throug', 8),\n",
       " ('computars', 8),\n",
       " ('reserch', 8),\n",
       " ('bullyed', 8),\n",
       " ('tierd', 8),\n",
       " ('bilding', 8),\n",
       " ('sertain', 8),\n",
       " ('neccesity', 8),\n",
       " ('theire', 8),\n",
       " ('seperating', 8),\n",
       " ('convienient', 8),\n",
       " ('peolple', 8),\n",
       " ('availible', 8),\n",
       " ('tecnology', 8),\n",
       " ('waisting', 8),\n",
       " ('@city1', 8),\n",
       " ('oppertunities', 8),\n",
       " ('resson', 8),\n",
       " ('intrested', 8),\n",
       " ('expirience', 8),\n",
       " ('useage', 8),\n",
       " ('deffinition', 8),\n",
       " ('othere', 8),\n",
       " ('convienent', 8),\n",
       " ('you;', 8),\n",
       " ('becomeing', 8),\n",
       " ('resturant', 8),\n",
       " ('ourselfs', 8),\n",
       " ('word-search', 7),\n",
       " ('inner-tubing', 7),\n",
       " ('soey', 7),\n",
       " ('can`t', 7),\n",
       " ('â\\x80\\x9cjohn', 7),\n",
       " ('donâ\\x80\\x99t', 7),\n",
       " ('iâ\\x80\\x99m', 7),\n",
       " ('\\x91m', 7),\n",
       " ('pacent', 7),\n",
       " ('finily', 7),\n",
       " ('paitient', 7),\n",
       " ('practicle', 7),\n",
       " ('blimbs', 7),\n",
       " ('hieght', 7),\n",
       " ('rocket-shaped', 7),\n",
       " ('obsticale', 7),\n",
       " ('dirible', 7),\n",
       " ('sacrafices', 7),\n",
       " ('habiscus', 7),\n",
       " ('narrarator', 7),\n",
       " ('rediculously', 7),\n",
       " ('inorder', 7),\n",
       " ('[the', 7),\n",
       " ('him;', 7),\n",
       " ('bestfriends', 7),\n",
       " ('segragation', 7),\n",
       " ('magaizines', 7),\n",
       " ('obsene', 7),\n",
       " ('material;', 7),\n",
       " ('differant', 7),\n",
       " ('musice', 7),\n",
       " ('offincive', 7),\n",
       " ('apporiate', 7),\n",
       " ('considerd', 7),\n",
       " ('affend', 7),\n",
       " ('provacative', 7),\n",
       " ('lanuage', 7),\n",
       " ('explict', 7),\n",
       " ('affended', 7),\n",
       " ('everynight', 7),\n",
       " ('talkes', 7),\n",
       " ('viloence', 7),\n",
       " ('launguage', 7),\n",
       " ('volgure', 7),\n",
       " ('gardian', 7),\n",
       " ('gaurdian', 7),\n",
       " ('gurdian', 7),\n",
       " ('opions', 7),\n",
       " ('libraian', 7),\n",
       " ('libraries;', 7),\n",
       " ('avaible', 7),\n",
       " ('unsensored', 7),\n",
       " ('discribe', 7),\n",
       " ('censhorship', 7),\n",
       " ('concluds', 7),\n",
       " ('minits', 7),\n",
       " ('negetive', 7),\n",
       " ('deffinitly', 7),\n",
       " ('intersting', 7),\n",
       " ('cusin', 7),\n",
       " ('inaproprate', 7),\n",
       " ('anyday', 7),\n",
       " ('dollers', 7),\n",
       " ('neccesary', 7),\n",
       " ('bigest', 7),\n",
       " ('wieght', 7),\n",
       " ('relitive', 7),\n",
       " ('knolage', 7),\n",
       " ('speek', 7),\n",
       " ('choise', 7),\n",
       " ('techknowlogy', 7),\n",
       " ('amusment', 7),\n",
       " ('stairing', 7),\n",
       " ('oppurtunities', 7),\n",
       " ('peopl', 7),\n",
       " ('absolutley', 7),\n",
       " ('minuts', 7),\n",
       " ('wrighting', 7),\n",
       " ('valueable', 7),\n",
       " ('experence', 7),\n",
       " ('everythings', 7),\n",
       " ('barly', 7),\n",
       " ('mentaly', 7),\n",
       " ('physicaly', 7),\n",
       " ('moniter', 7),\n",
       " ('agreat', 7),\n",
       " ('definitly', 7),\n",
       " ('highschoolers', 7),\n",
       " ('totaly', 7),\n",
       " ('becus', 7),\n",
       " ('simplier', 7),\n",
       " ('cought', 7),\n",
       " ('normaly', 7),\n",
       " ('lazyness', 7),\n",
       " ('apreciate', 7),\n",
       " ('oponion', 7),\n",
       " ('thas', 7),\n",
       " ('acually', 7),\n",
       " ('beatiful', 7),\n",
       " ('samething', 7),\n",
       " ('obiese', 7),\n",
       " ('positve', 7),\n",
       " ('cordenation', 7),\n",
       " ('promblems', 7),\n",
       " ('relie', 7),\n",
       " ('messanger', 7),\n",
       " ('obeseity', 7),\n",
       " ('injoy', 7),\n",
       " ('hadnt', 7),\n",
       " ('cyberbulling', 7),\n",
       " ('comptuers', 7),\n",
       " ('famly', 7),\n",
       " ('physicly', 7),\n",
       " ('recieves', 7),\n",
       " ('anyting', 7),\n",
       " ('well;', 7),\n",
       " ('studys', 7),\n",
       " ('opition', 7),\n",
       " ('senery', 7),\n",
       " ('deffinately', 7),\n",
       " ('relatioship', 6),\n",
       " ('patiencent', 6),\n",
       " ('â\\x80\\x99', 6),\n",
       " ('pacents', 6),\n",
       " ('patince', 6),\n",
       " ('patint', 6),\n",
       " ('pacint', 6),\n",
       " ('swivle', 6),\n",
       " ('dirigibels', 6),\n",
       " ('blimb', 6),\n",
       " ('glassed-in', 6),\n",
       " ('flammible', 6),\n",
       " ('poped', 6),\n",
       " ('appriciative', 6),\n",
       " ('imagrents', 6),\n",
       " ('carrers', 6),\n",
       " ('sacraficed', 6),\n",
       " ('gesse', 6),\n",
       " ('say;', 6),\n",
       " ('at-at-', 6),\n",
       " ('budds', 6),\n",
       " ('hibicus', 6),\n",
       " ('competive', 6),\n",
       " ('journy', 6),\n",
       " ('perservere', 6),\n",
       " ('[he]', 6),\n",
       " ('abanded', 6),\n",
       " ('whitch', 6),\n",
       " ('listend', 6),\n",
       " ('fufill', 6),\n",
       " ('inapporite', 6),\n",
       " ('materal', 6),\n",
       " ('aurthor', 6),\n",
       " ('librairy', 6),\n",
       " ('rateing', 6),\n",
       " ('materail', 6),\n",
       " ('offenisve', 6),\n",
       " ('audiance', 6),\n",
       " ('languge', 6),\n",
       " ('theirselves', 6),\n",
       " ('concent', 6),\n",
       " ('rascist', 6),\n",
       " ('someting', 6),\n",
       " ('carefull', 6),\n",
       " ('seperates', 6),\n",
       " ('becaused', 6),\n",
       " ('complane', 6),\n",
       " ('guidence', 6),\n",
       " ('person;', 6),\n",
       " ('pornagraphic', 6),\n",
       " ('as;', 6),\n",
       " ('dangerouse', 6),\n",
       " ('dispite', 6),\n",
       " ('about;', 6),\n",
       " ('are;', 6),\n",
       " ('experiance', 6),\n",
       " ('hopless', 6),\n",
       " ('strenght', 6),\n",
       " ('exeed', 6),\n",
       " ('eventualy', 6),\n",
       " ('contraversy', 6),\n",
       " ('peacful', 6),\n",
       " ('lastley', 6),\n",
       " ('ecspecially', 6),\n",
       " ('negativly', 6),\n",
       " ('inapproiate', 6),\n",
       " ('firend', 6),\n",
       " ('definatley', 6),\n",
       " ('acces', 6),\n",
       " ('ahve', 6),\n",
       " ('writeing', 6),\n",
       " ('fmaily', 6),\n",
       " ('heared', 6),\n",
       " ('truley', 6),\n",
       " ('relgion', 6),\n",
       " ('desicion', 6),\n",
       " ('quickley', 6),\n",
       " ('intrests', 6),\n",
       " ('there;', 6),\n",
       " ('neccessarily', 6),\n",
       " ('entertaning', 6),\n",
       " ('realtives', 6),\n",
       " ('oportunity', 6),\n",
       " ('powerpoints', 6),\n",
       " ('realitives', 6),\n",
       " ('forein', 6),\n",
       " ('oppinon', 6),\n",
       " ('technoligy', 6),\n",
       " ('moveing', 6),\n",
       " ('beutiful', 6),\n",
       " ('inapropreate', 6),\n",
       " ('wheight', 6),\n",
       " ('docters', 6),\n",
       " ('afect', 6),\n",
       " ('necesity', 6),\n",
       " ('restraunt', 6),\n",
       " ('findout', 6),\n",
       " ('exellent', 6),\n",
       " ('espicially', 6),\n",
       " ('wonderfull', 6),\n",
       " ('exersicing', 6),\n",
       " ('conected', 6),\n",
       " ('obeisity', 6),\n",
       " ('nessesary', 6),\n",
       " ('websit', 6),\n",
       " ('socitey', 6),\n",
       " ('apointment', 6),\n",
       " ('exausted', 6),\n",
       " ('harrased', 6),\n",
       " ('whatch', 6),\n",
       " ('somepeople', 6),\n",
       " ('attatched', 6),\n",
       " ('bacause', 6),\n",
       " ('anouther', 6),\n",
       " ('adicting', 6),\n",
       " ('websights', 6),\n",
       " ('youself', 6),\n",
       " ('thease', 6),\n",
       " ('piont', 6),\n",
       " ('lern', 6),\n",
       " ('exercizing', 6),\n",
       " ('parrents', 6),\n",
       " ('convinent', 6),\n",
       " ('reconize', 6),\n",
       " ('ohter', 6),\n",
       " ('palying', 6),\n",
       " ('easer', 6),\n",
       " ('showes', 6),\n",
       " ('thngs', 6),\n",
       " ('comunicating', 6),\n",
       " ('forgein', 6),\n",
       " ('helpul', 6),\n",
       " ('myspce', 6),\n",
       " ('defenetly', 6),\n",
       " ('wacth', 6),\n",
       " ('friends;', 6),\n",
       " ('wasteing', 6),\n",
       " ('goind', 6),\n",
       " ('facebooks', 6),\n",
       " ('laughters', 5),\n",
       " ('other;', 5),\n",
       " ('boyfriend/girlfriend', 5),\n",
       " ('farted', 5),\n",
       " ('whate', 5),\n",
       " ('rollor', 5),\n",
       " ('cernal', 5),\n",
       " ('hospitle', 5),\n",
       " ('replyed', 5),\n",
       " ('couldnâ\\x80\\x99t', 5),\n",
       " ('payshent', 5),\n",
       " ('thatâ\\x80\\x99s', 5),\n",
       " ('patiente', 5),\n",
       " ('didnâ\\x80\\x99t', 5),\n",
       " ('moorning', 5),\n",
       " ('buiders', 5),\n",
       " ('zepplins', 5),\n",
       " ('steelframe', 5),\n",
       " ('densley', 5),\n",
       " ('teathered', 5),\n",
       " ('was;', 5),\n",
       " ('thousand-', 5),\n",
       " ('itself;', 5),\n",
       " ('safetly', 5),\n",
       " ('memmoir', 5),\n",
       " ('paragraf', 5),\n",
       " ('happness', 5),\n",
       " ('familey', 5),\n",
       " ('nonstalgic', 5),\n",
       " ('parents]', 5),\n",
       " ('memor', 5),\n",
       " ('grattitude', 5),\n",
       " ('imagrants', 5),\n",
       " ('immagrated', 5),\n",
       " ('blonde-brick', 5),\n",
       " ('at-at', 5),\n",
       " ('saeng\\x91s', 5),\n",
       " ('lisence', 5),\n",
       " ('faeng-noi', 5),\n",
       " ('buding', 5),\n",
       " ('strenghth', 5),\n",
       " ('is-it', 5),\n",
       " ('[1', 5),\n",
       " ('perservering', 5),\n",
       " ('rollar', 5),\n",
       " ('thrist', 5),\n",
       " ('snake-it', 5),\n",
       " ('afected', 5),\n",
       " ('abandond', 5),\n",
       " ('\\x91s', 5),\n",
       " ('snake-', 5),\n",
       " ('diamondback-blocked', 5),\n",
       " ('cylist', 5),\n",
       " ('\\x91town', 5),\n",
       " ('luckly', 5),\n",
       " ('apporpriate', 5),\n",
       " ('like;', 5),\n",
       " ('memeries', 5),\n",
       " ('cursewords', 5),\n",
       " ('book/magazine', 5),\n",
       " ('worng', 5),\n",
       " ('complian', 5),\n",
       " ('meterial', 5),\n",
       " ('accomidate', 5),\n",
       " ('bookes', 5),\n",
       " ('unapropriate', 5),\n",
       " ('non-', 5),\n",
       " ('lable', 5),\n",
       " ('eightteen', 5),\n",
       " ('lauguage', 5),\n",
       " ('shelf--', 5),\n",
       " ('libarian', 5),\n",
       " ('worrie', 5),\n",
       " ('provokative', 5),\n",
       " ('sheleves', 5),\n",
       " ('famliy', 5),\n",
       " ('accomadate', 5),\n",
       " ('magizens', 5),\n",
       " ('offenive', 5),\n",
       " ('read;', 5),\n",
       " ('grusome', 5),\n",
       " ('censered', 5),\n",
       " ('right;', 5),\n",
       " ('inmature', 5),\n",
       " ('offenseive', 5),\n",
       " ('sensorship', 5),\n",
       " ('thing;', 5),\n",
       " ('sheild', 5),\n",
       " ('regaurdless', 5),\n",
       " ('censoship', 5),\n",
       " ('neccasary', 5),\n",
       " ('intersted', 5),\n",
       " ('privelages', 5),\n",
       " ('againts', 5),\n",
       " ('parnets', 5),\n",
       " ('athiest', 5),\n",
       " ('contraversial', 5),\n",
       " ('offensve', 5),\n",
       " ('esle', 5),\n",
       " ('censoreship', 5),\n",
       " ('expierences', 5),\n",
       " ('instence', 5),\n",
       " ('differnet', 5),\n",
       " ('exceptable', 5),\n",
       " ('oppenion', 5),\n",
       " ('catagorized', 5),\n",
       " ('inforced', 5),\n",
       " ('not;', 5),\n",
       " ('innocense', 5),\n",
       " ('libarys', 5),\n",
       " ('censorhip', 5),\n",
       " ('pornagraphy', 5),\n",
       " ('diffrence', 5),\n",
       " ('writter', 5),\n",
       " ('thigs', 5),\n",
       " ('hungery', 5),\n",
       " ('preasure', 5),\n",
       " ('tyring', 5),\n",
       " ('lastely', 5),\n",
       " ('exept', 5),\n",
       " ('proble', 5),\n",
       " ('faild', 5),\n",
       " ('pleasent', 5),\n",
       " ('aparent', 5),\n",
       " ('bussiness', 5),\n",
       " ('privelage', 5),\n",
       " ('interfer', 5),\n",
       " ('concider', 5),\n",
       " ('akward', 5),\n",
       " ('biulding', 5),\n",
       " ('explaing', 5),\n",
       " ('buisiness', 5),\n",
       " ('sociaty', 5),\n",
       " ('to;', 5),\n",
       " ('libarary', 5),\n",
       " ('therfore', 5),\n",
       " ('pysical', 5),\n",
       " ('school;', 5),\n",
       " ('benefical', 5),\n",
       " ('hault', 5),\n",
       " ('seting', 5),\n",
       " ('seens', 5),\n",
       " ('engry', 5),\n",
       " ('aroung', 5),\n",
       " ('ecersise', 5),\n",
       " ('difrent', 5),\n",
       " ('easyier', 5),\n",
       " ('actuly', 5),\n",
       " ('conect', 5),\n",
       " ('adictive', 5),\n",
       " ('telivision', 5),\n",
       " ('thaught', 5),\n",
       " ('perents', 5),\n",
       " ('us;', 5),\n",
       " ('lisen', 5),\n",
       " ('liveing', 5),\n",
       " ('distroyed', 5),\n",
       " ('intenet', 5),\n",
       " ('withought', 5),\n",
       " ('penpals', 5),\n",
       " ('neice', 5),\n",
       " ('excersicing', 5),\n",
       " ('cornation', 5),\n",
       " ('pregnacy', 5),\n",
       " ('outragous', 5),\n",
       " ('greates', 5),\n",
       " ('leaft', 5),\n",
       " ('explaines', 5),\n",
       " ('awnser', 5),\n",
       " ('exircise', 5),\n",
       " ('wnat', 5),\n",
       " ('familier', 5),\n",
       " ('begginning', 5),\n",
       " ('neccessity', 5),\n",
       " ('busnesses', 5),\n",
       " ('definetely', 5),\n",
       " ('argueing', 5),\n",
       " ('importent', 5),\n",
       " ('practicly', 5),\n",
       " ('constanly', 5),\n",
       " ('out;', 5),\n",
       " ('acidents', 5),\n",
       " ('travling', 5),\n",
       " ('computers;', 5),\n",
       " ('aldults', 5),\n",
       " ('recipies', 5),\n",
       " ('nessisary', 5),\n",
       " ('societys', 5),\n",
       " ('on;', 5),\n",
       " ('positivly', 5),\n",
       " ('accesible', 5),\n",
       " ('allready', 5),\n",
       " ('eaiser', 5),\n",
       " ('scientest', 5),\n",
       " ('corrdination', 5),\n",
       " ('book;', 5),\n",
       " ('preditors', 5),\n",
       " ('proffesor', 5),\n",
       " ('solitare', 5),\n",
       " ('before;', 5),\n",
       " ('opertunities', 5),\n",
       " ('happends', 5),\n",
       " ('-peter', 5),\n",
       " ('nessecary', 5),\n",
       " ('understant', 5),\n",
       " ('childre', 5),\n",
       " ('coumputers', 5),\n",
       " ('bad;', 5),\n",
       " ('youve', 5),\n",
       " ('cancled', 5),\n",
       " ('choosen', 5),\n",
       " ('vocab', 5),\n",
       " ('anythin', 5),\n",
       " ('exsist', 5),\n",
       " ('thnk', 5),\n",
       " ('ocmputer', 5),\n",
       " ('teached', 5),\n",
       " ('compuer', 5),\n",
       " ('libery', 5),\n",
       " ('softwear', 5),\n",
       " ('dosnt', 5),\n",
       " ('olny', 5),\n",
       " ('futhermore', 5),\n",
       " ('resturants', 5),\n",
       " ('orginized', 5),\n",
       " ('hastle', 5),\n",
       " ('cyberbullied', 5),\n",
       " ('ti-rent', 4),\n",
       " ('towl', 4),\n",
       " ('fricken', 4),\n",
       " ('toung', 4),\n",
       " ('all-peter', 4),\n",
       " ('girlfreind', 4),\n",
       " ('laugh;', 4),\n",
       " ('lt;peter', 4),\n",
       " ('cout', 4),\n",
       " ('step-dad', 4),\n",
       " ('kickline', 4),\n",
       " ('surgey', 4),\n",
       " ('minuites', 4),\n",
       " ('iâ\\x80\\x99ll', 4),\n",
       " ('patientce', 4),\n",
       " ('grocerys', 4),\n",
       " ('pations', 4),\n",
       " ('coner', 4),\n",
       " ('sharae', 4),\n",
       " ('atime', 4),\n",
       " ('marrie', 4),\n",
       " ('chees', 4),\n",
       " ('cuzins', 4),\n",
       " ('patence', 4),\n",
       " ('madichian', 4),\n",
       " ('compition', 4),\n",
       " ('dirrigible', 4),\n",
       " ('flamibility', 4),\n",
       " ('185-foot', 4),\n",
       " ('baloon', 4),\n",
       " ('dirgible', 4),\n",
       " ('obstacal', 4),\n",
       " ('dirigibe', 4),\n",
       " ('sixty-thousand', 4),\n",
       " ('obstecles', 4),\n",
       " ('building]', 4),\n",
       " ('attemp', 4),\n",
       " ('refule', 4),\n",
       " ('nature;', 4),\n",
       " ('architecs', 4),\n",
       " ('architechs', 4),\n",
       " ('strees', 4),\n",
       " ('homyness', 4),\n",
       " ('undefind', 4),\n",
       " ...]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = makeEssayArr(replaceCommon=True, removeSW=False)\n",
    "vocab = build_vocab(sentences)\n",
    "check_coverage(vocab,glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39911"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12701,)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setID():\n",
    "    id=[]\n",
    "    for index, df in enumerate(data.essay):\n",
    "        id.append(index)\n",
    "    return id\n",
    "data['id']=setID()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2vec(essay_id):\n",
    "    size = len(sentences[essay_id])\n",
    "    found = 0\n",
    "    embedding = np.zeros(300,dtype=\"float32\")\n",
    "    for word in sentences[essay_id]:\n",
    "        if word in glove:\n",
    "            found+=1\n",
    "            embedding += glove[word]\n",
    "    embedding = np.divide(embedding,found)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['embeddings'] = data['id'].apply(lambda x: convert2vec(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "      <th>id</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12971</th>\n",
       "      <td>21626</td>\n",
       "      <td>8</td>\n",
       "      <td>In most stories mothers and daughters are eit...</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12696</td>\n",
       "      <td>[-0.122094214, 0.09524214, -0.053078335, -0.10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12972</th>\n",
       "      <td>21628</td>\n",
       "      <td>8</td>\n",
       "      <td>I never understood the meaning laughter is th...</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12697</td>\n",
       "      <td>[-0.124869, 0.091405176, -0.045465734, -0.0824...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12973</th>\n",
       "      <td>21629</td>\n",
       "      <td>8</td>\n",
       "      <td>When you laugh, is @CAPS5 out of habit, or is ...</td>\n",
       "      <td>20</td>\n",
       "      <td>26</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12698</td>\n",
       "      <td>[-0.09099377, 0.08082998, -0.04143547, -0.1448...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12974</th>\n",
       "      <td>21630</td>\n",
       "      <td>8</td>\n",
       "      <td>Trippin' on fen...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12699</td>\n",
       "      <td>[-0.10467531, 0.061440624, -0.046567135, -0.09...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12975</th>\n",
       "      <td>21633</td>\n",
       "      <td>8</td>\n",
       "      <td>Many people believe that laughter can improve...</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12700</td>\n",
       "      <td>[-0.14170836, 0.11989354, -0.034562092, -0.143...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       essay_id  essay_set                                              essay  \\\n",
       "12971     21626          8   In most stories mothers and daughters are eit...   \n",
       "12972     21628          8   I never understood the meaning laughter is th...   \n",
       "12973     21629          8  When you laugh, is @CAPS5 out of habit, or is ...   \n",
       "12974     21630          8                                 Trippin' on fen...   \n",
       "12975     21633          8   Many people believe that laughter can improve...   \n",
       "\n",
       "       rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "12971              17              18             NaN             35   \n",
       "12972              15              17             NaN             32   \n",
       "12973              20              26            40.0             40   \n",
       "12974              20              20             NaN             40   \n",
       "12975              20              20             NaN             40   \n",
       "\n",
       "       rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait5  \\\n",
       "12971             NaN             NaN            NaN  ...            4.0   \n",
       "12972             NaN             NaN            NaN  ...            4.0   \n",
       "12973             NaN             NaN            NaN  ...            5.0   \n",
       "12974             NaN             NaN            NaN  ...            4.0   \n",
       "12975             NaN             NaN            NaN  ...            4.0   \n",
       "\n",
       "       rater2_trait6  rater3_trait1  rater3_trait2  rater3_trait3  \\\n",
       "12971            3.0            NaN            NaN            NaN   \n",
       "12972            3.0            NaN            NaN            NaN   \n",
       "12973            5.0            4.0            4.0            4.0   \n",
       "12974            4.0            NaN            NaN            NaN   \n",
       "12975            4.0            NaN            NaN            NaN   \n",
       "\n",
       "       rater3_trait4  rater3_trait5  rater3_trait6     id  \\\n",
       "12971            NaN            NaN            NaN  12696   \n",
       "12972            NaN            NaN            NaN  12697   \n",
       "12973            4.0            4.0            4.0  12698   \n",
       "12974            NaN            NaN            NaN  12699   \n",
       "12975            NaN            NaN            NaN  12700   \n",
       "\n",
       "                                              embeddings  \n",
       "12971  [-0.122094214, 0.09524214, -0.053078335, -0.10...  \n",
       "12972  [-0.124869, 0.091405176, -0.045465734, -0.0824...  \n",
       "12973  [-0.09099377, 0.08082998, -0.04143547, -0.1448...  \n",
       "12974  [-0.10467531, 0.061440624, -0.046567135, -0.09...  \n",
       "12975  [-0.14170836, 0.11989354, -0.034562092, -0.143...  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "      <th>id</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>final_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.1852118, 0.06012019, -0.029879754, -0.1156...</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.1839048, 0.10882854, -0.027344918, -0.1496...</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>[-0.16473815, 0.106056325, 0.0016457571, -0.16...</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>[-0.17554235, 0.11150211, -0.044427224, -0.169...</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>[-0.18809283, 0.108385086, -0.053500302, -0.16...</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0               4               4             NaN              8   \n",
       "1               5               4             NaN              9   \n",
       "2               4               3             NaN              7   \n",
       "3               5               5             NaN             10   \n",
       "4               4               4             NaN              8   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait6  \\\n",
       "0             NaN             NaN            NaN  ...            NaN   \n",
       "1             NaN             NaN            NaN  ...            NaN   \n",
       "2             NaN             NaN            NaN  ...            NaN   \n",
       "3             NaN             NaN            NaN  ...            NaN   \n",
       "4             NaN             NaN            NaN  ...            NaN   \n",
       "\n",
       "   rater3_trait1  rater3_trait2  rater3_trait3  rater3_trait4  rater3_trait5  \\\n",
       "0            NaN            NaN            NaN            NaN            NaN   \n",
       "1            NaN            NaN            NaN            NaN            NaN   \n",
       "2            NaN            NaN            NaN            NaN            NaN   \n",
       "3            NaN            NaN            NaN            NaN            NaN   \n",
       "4            NaN            NaN            NaN            NaN            NaN   \n",
       "\n",
       "   rater3_trait6  id                                         embeddings  \\\n",
       "0            NaN   0  [-0.1852118, 0.06012019, -0.029879754, -0.1156...   \n",
       "1            NaN   1  [-0.1839048, 0.10882854, -0.027344918, -0.1496...   \n",
       "2            NaN   2  [-0.16473815, 0.106056325, 0.0016457571, -0.16...   \n",
       "3            NaN   3  [-0.17554235, 0.11150211, -0.044427224, -0.169...   \n",
       "4            NaN   4  [-0.18809283, 0.108385086, -0.053500302, -0.16...   \n",
       "\n",
       "   final_score  \n",
       "0         40.0  \n",
       "1         45.0  \n",
       "2         35.0  \n",
       "3         50.0  \n",
       "4         40.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nndata = data.copy()\n",
    "nndata['final_score'] = nndata.apply(lambda row: row['domain1_score'] if row['essay_set']!=2 else row['domain1_score'] + row['domain2_score'], axis=1)\n",
    "mul = [5,6,20,20,15,15,2,1]\n",
    "nndata['final_score'] = nndata.apply(lambda row: row['final_score']*mul[row['essay_set']-1], axis=1)\n",
    "# nndata.groupby('essay_set').describe()\n",
    "nndata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10902     8.8\n",
       "2234      4.8\n",
       "1625     10.0\n",
       "9184      6.0\n",
       "8035      3.0\n",
       "8457      6.0\n",
       "838       7.0\n",
       "1478      6.0\n",
       "8726      3.0\n",
       "639       8.0\n",
       "9217      9.0\n",
       "10103    12.0\n",
       "1949      9.6\n",
       "3102      8.4\n",
       "9065      9.0\n",
       "83        6.0\n",
       "623       7.0\n",
       "10779     8.8\n",
       "3962      8.0\n",
       "4630     12.0\n",
       "5176      8.0\n",
       "8246      6.0\n",
       "12243     7.6\n",
       "9987     12.0\n",
       "148       8.0\n",
       "12531     7.6\n",
       "6212      0.0\n",
       "8898      6.0\n",
       "12955     7.2\n",
       "1096      8.0\n",
       "         ... \n",
       "10804     7.6\n",
       "10666    12.0\n",
       "6029      4.0\n",
       "11846     9.2\n",
       "1895      7.2\n",
       "1657      8.0\n",
       "3623      4.0\n",
       "2593      9.6\n",
       "7926     12.0\n",
       "5036     12.0\n",
       "4002      8.0\n",
       "9621      6.0\n",
       "4536      4.0\n",
       "6878      0.0\n",
       "12579     9.2\n",
       "11726     7.6\n",
       "9429      9.0\n",
       "5554      0.0\n",
       "1813      9.6\n",
       "7256      6.0\n",
       "11671     7.6\n",
       "11004     3.6\n",
       "8389      6.0\n",
       "10391     9.0\n",
       "8446     12.0\n",
       "10101    12.0\n",
       "9677      0.0\n",
       "11451     8.0\n",
       "361      11.0\n",
       "4435      8.0\n",
       "Name: final_score, Length: 12701, dtype: float64"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nndata = nndata.loc[(nndata['essay_set']==4)]\n",
    "nndata = nndata[['embeddings', 'final_score']]\n",
    "train_data = nndata.sample(frac=1)\n",
    "test_data = nndata.drop(train_data.index)\n",
    "train_labels = train_data.pop('final_score')\n",
    "test_labels = test_data.pop('final_score')\n",
    "\n",
    "train_labels = train_labels/5\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(512, activation=tf.nn.relu, input_shape=np.shape(train_data.iloc[0]['embeddings'])),\n",
    "    layers.Dense(128, activation=tf.nn.relu),\n",
    "    layers.Dense(1,activation=\"linear\")\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "  model.compile(loss='mean_squared_error',\n",
    "                optimizer=optimizer,\n",
    "                metrics=[ 'mean_squared_error'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_48 (Dense)             (None, 512)               154112    \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 219,905\n",
      "Trainable params: 219,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_arr = np.zeros((train_data.size,300),dtype=\"float32\")\n",
    "for i, emb in enumerate(train_data['embeddings']):\n",
    "    train_data_arr[i]=emb\n",
    "train_labels_arr = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10160 samples, validate on 2541 samples\n",
      "Epoch 1/100\n",
      "10160/10160 [==============================] - 2s 212us/sample - loss: 6.9967 - mean_squared_error: 6.9967 - val_loss: 7.0140 - val_mean_squared_error: 7.0140\n",
      "Epoch 2/100\n",
      "10160/10160 [==============================] - 2s 167us/sample - loss: 5.6547 - mean_squared_error: 5.6547 - val_loss: 5.7928 - val_mean_squared_error: 5.7928\n",
      "Epoch 3/100\n",
      "10160/10160 [==============================] - 2s 163us/sample - loss: 5.1505 - mean_squared_error: 5.1505 - val_loss: 5.7897 - val_mean_squared_error: 5.7897\n",
      "Epoch 4/100\n",
      "10160/10160 [==============================] - 2s 178us/sample - loss: 4.7713 - mean_squared_error: 4.7713 - val_loss: 5.4972 - val_mean_squared_error: 5.4972\n",
      "Epoch 5/100\n",
      "10160/10160 [==============================] - 2s 167us/sample - loss: 4.5653 - mean_squared_error: 4.5653 - val_loss: 4.5443 - val_mean_squared_error: 4.5443\n",
      "Epoch 6/100\n",
      "10160/10160 [==============================] - 2s 172us/sample - loss: 4.3775 - mean_squared_error: 4.3775 - val_loss: 4.3517 - val_mean_squared_error: 4.3517\n",
      "Epoch 7/100\n",
      "10160/10160 [==============================] - 2s 176us/sample - loss: 4.2241 - mean_squared_error: 4.2241 - val_loss: 4.1282 - val_mean_squared_error: 4.1282\n",
      "Epoch 8/100\n",
      "10160/10160 [==============================] - 2s 177us/sample - loss: 4.0755 - mean_squared_error: 4.0755 - val_loss: 4.0927 - val_mean_squared_error: 4.0927\n",
      "Epoch 9/100\n",
      "10160/10160 [==============================] - 2s 213us/sample - loss: 3.9393 - mean_squared_error: 3.9393 - val_loss: 4.8029 - val_mean_squared_error: 4.8029\n",
      "Epoch 10/100\n",
      "10160/10160 [==============================] - 2s 195us/sample - loss: 3.8102 - mean_squared_error: 3.8102 - val_loss: 6.0144 - val_mean_squared_error: 6.0144\n",
      "Epoch 11/100\n",
      "10160/10160 [==============================] - 2s 211us/sample - loss: 3.7309 - mean_squared_error: 3.7309 - val_loss: 5.2410 - val_mean_squared_error: 5.2410\n",
      "Epoch 12/100\n",
      "10160/10160 [==============================] - 2s 181us/sample - loss: 3.6165 - mean_squared_error: 3.6165 - val_loss: 4.2969 - val_mean_squared_error: 4.2969\n",
      "Epoch 13/100\n",
      "10160/10160 [==============================] - 2s 198us/sample - loss: 3.5513 - mean_squared_error: 3.5513 - val_loss: 4.7431 - val_mean_squared_error: 4.7431\n",
      "Epoch 14/100\n",
      "10160/10160 [==============================] - 2s 177us/sample - loss: 3.4487 - mean_squared_error: 3.4487 - val_loss: 4.2942 - val_mean_squared_error: 4.2942\n",
      "Epoch 15/100\n",
      "10160/10160 [==============================] - 2s 187us/sample - loss: 3.3692 - mean_squared_error: 3.3692 - val_loss: 3.7586 - val_mean_squared_error: 3.7586\n",
      "Epoch 16/100\n",
      "10160/10160 [==============================] - 2s 199us/sample - loss: 3.3278 - mean_squared_error: 3.3278 - val_loss: 4.1932 - val_mean_squared_error: 4.1932\n",
      "Epoch 17/100\n",
      "10160/10160 [==============================] - 2s 184us/sample - loss: 3.2377 - mean_squared_error: 3.2377 - val_loss: 5.1683 - val_mean_squared_error: 5.1683\n",
      "Epoch 18/100\n",
      "10160/10160 [==============================] - 2s 191us/sample - loss: 3.2298 - mean_squared_error: 3.2298 - val_loss: 4.4749 - val_mean_squared_error: 4.4749\n",
      "Epoch 19/100\n",
      "10160/10160 [==============================] - 2s 183us/sample - loss: 3.0776 - mean_squared_error: 3.0776 - val_loss: 3.9095 - val_mean_squared_error: 3.9095\n",
      "Epoch 20/100\n",
      "10160/10160 [==============================] - 2s 175us/sample - loss: 3.0472 - mean_squared_error: 3.0472 - val_loss: 5.5813 - val_mean_squared_error: 5.5813\n",
      "Epoch 21/100\n",
      "10160/10160 [==============================] - 2s 157us/sample - loss: 2.9554 - mean_squared_error: 2.9554 - val_loss: 3.6222 - val_mean_squared_error: 3.6222\n",
      "Epoch 22/100\n",
      "10160/10160 [==============================] - 2s 162us/sample - loss: 2.9178 - mean_squared_error: 2.9178 - val_loss: 3.5209 - val_mean_squared_error: 3.5209\n",
      "Epoch 23/100\n",
      "10160/10160 [==============================] - 2s 157us/sample - loss: 2.8623 - mean_squared_error: 2.8623 - val_loss: 4.3025 - val_mean_squared_error: 4.3025\n",
      "Epoch 24/100\n",
      "10160/10160 [==============================] - 2s 160us/sample - loss: 2.8489 - mean_squared_error: 2.8489 - val_loss: 4.9170 - val_mean_squared_error: 4.9170\n",
      "Epoch 25/100\n",
      "10160/10160 [==============================] - 2s 161us/sample - loss: 2.7736 - mean_squared_error: 2.7736 - val_loss: 3.7997 - val_mean_squared_error: 3.7997\n",
      "Epoch 26/100\n",
      "10160/10160 [==============================] - 2s 167us/sample - loss: 2.7404 - mean_squared_error: 2.7404 - val_loss: 3.8171 - val_mean_squared_error: 3.8171\n",
      "Epoch 27/100\n",
      "10160/10160 [==============================] - 2s 188us/sample - loss: 2.6782 - mean_squared_error: 2.6782 - val_loss: 3.8582 - val_mean_squared_error: 3.8582\n",
      "Epoch 28/100\n",
      "10160/10160 [==============================] - 2s 161us/sample - loss: 2.6457 - mean_squared_error: 2.6457 - val_loss: 4.2235 - val_mean_squared_error: 4.2235\n",
      "Epoch 29/100\n",
      "10160/10160 [==============================] - 2s 162us/sample - loss: 2.5901 - mean_squared_error: 2.5901 - val_loss: 4.7470 - val_mean_squared_error: 4.7470\n",
      "Epoch 30/100\n",
      "10160/10160 [==============================] - 2s 202us/sample - loss: 2.5735 - mean_squared_error: 2.5735 - val_loss: 4.6192 - val_mean_squared_error: 4.6192\n",
      "Epoch 31/100\n",
      "10160/10160 [==============================] - 2s 170us/sample - loss: 2.5340 - mean_squared_error: 2.5340 - val_loss: 3.7907 - val_mean_squared_error: 3.7907\n",
      "Epoch 32/100\n",
      "10160/10160 [==============================] - 2s 162us/sample - loss: 2.4877 - mean_squared_error: 2.4877 - val_loss: 3.5613 - val_mean_squared_error: 3.5613\n",
      "Epoch 33/100\n",
      "10160/10160 [==============================] - 2s 165us/sample - loss: 2.4232 - mean_squared_error: 2.4232 - val_loss: 3.8767 - val_mean_squared_error: 3.8767\n",
      "Epoch 34/100\n",
      "10160/10160 [==============================] - 2s 162us/sample - loss: 2.4135 - mean_squared_error: 2.4135 - val_loss: 4.4649 - val_mean_squared_error: 4.4649\n",
      "Epoch 35/100\n",
      "10160/10160 [==============================] - 2s 162us/sample - loss: 2.3781 - mean_squared_error: 2.3781 - val_loss: 4.5654 - val_mean_squared_error: 4.5654\n",
      "Epoch 36/100\n",
      "10160/10160 [==============================] - 2s 162us/sample - loss: 2.3487 - mean_squared_error: 2.3487 - val_loss: 4.0577 - val_mean_squared_error: 4.0577\n",
      "Epoch 37/100\n",
      "10160/10160 [==============================] - 2s 190us/sample - loss: 2.3146 - mean_squared_error: 2.3146 - val_loss: 3.4797 - val_mean_squared_error: 3.4797\n",
      "Epoch 38/100\n",
      "10160/10160 [==============================] - 2s 162us/sample - loss: 2.2521 - mean_squared_error: 2.2521 - val_loss: 3.4007 - val_mean_squared_error: 3.4007\n",
      "Epoch 39/100\n",
      "10160/10160 [==============================] - 2s 161us/sample - loss: 2.2248 - mean_squared_error: 2.2248 - val_loss: 3.4809 - val_mean_squared_error: 3.4809\n",
      "Epoch 40/100\n",
      "10160/10160 [==============================] - 2s 162us/sample - loss: 2.1905 - mean_squared_error: 2.1905 - val_loss: 3.6148 - val_mean_squared_error: 3.6148\n",
      "Epoch 41/100\n",
      "10160/10160 [==============================] - 2s 162us/sample - loss: 2.1638 - mean_squared_error: 2.1638 - val_loss: 3.5603 - val_mean_squared_error: 3.5603\n",
      "Epoch 42/100\n",
      "10160/10160 [==============================] - 2s 162us/sample - loss: 2.1424 - mean_squared_error: 2.1424 - val_loss: 3.5292 - val_mean_squared_error: 3.5292\n",
      "Epoch 43/100\n",
      "10160/10160 [==============================] - 2s 163us/sample - loss: 2.0868 - mean_squared_error: 2.0868 - val_loss: 3.7264 - val_mean_squared_error: 3.7264\n",
      "Epoch 44/100\n",
      "10160/10160 [==============================] - 2s 161us/sample - loss: 2.0482 - mean_squared_error: 2.0482 - val_loss: 3.5159 - val_mean_squared_error: 3.5159\n",
      "Epoch 45/100\n",
      "10160/10160 [==============================] - 2s 188us/sample - loss: 2.0239 - mean_squared_error: 2.0239 - val_loss: 4.0759 - val_mean_squared_error: 4.0759\n",
      "Epoch 46/100\n",
      "10160/10160 [==============================] - 2s 166us/sample - loss: 2.0074 - mean_squared_error: 2.0074 - val_loss: 4.2464 - val_mean_squared_error: 4.2464\n",
      "Epoch 47/100\n",
      "10160/10160 [==============================] - 2s 163us/sample - loss: 1.9709 - mean_squared_error: 1.9709 - val_loss: 3.9947 - val_mean_squared_error: 3.9947\n",
      "Epoch 48/100\n",
      "10160/10160 [==============================] - 1s 145us/sample - loss: 1.9406 - mean_squared_error: 1.9406 - val_loss: 3.8989 - val_mean_squared_error: 3.8989\n",
      "Epoch 49/100\n",
      "10160/10160 [==============================] - 1s 139us/sample - loss: 1.8983 - mean_squared_error: 1.8983 - val_loss: 3.5995 - val_mean_squared_error: 3.5995\n",
      "Epoch 50/100\n",
      "10160/10160 [==============================] - 1s 141us/sample - loss: 1.8435 - mean_squared_error: 1.8435 - val_loss: 3.7778 - val_mean_squared_error: 3.7778\n",
      "Epoch 51/100\n",
      "10160/10160 [==============================] - 1s 139us/sample - loss: 1.8254 - mean_squared_error: 1.8254 - val_loss: 3.7672 - val_mean_squared_error: 3.7672\n",
      "Epoch 52/100\n",
      "10160/10160 [==============================] - 1s 146us/sample - loss: 1.8061 - mean_squared_error: 1.8061 - val_loss: 3.6753 - val_mean_squared_error: 3.6753\n",
      "Epoch 53/100\n",
      "10160/10160 [==============================] - 1s 141us/sample - loss: 1.7838 - mean_squared_error: 1.7838 - val_loss: 3.7260 - val_mean_squared_error: 3.7260\n",
      "Epoch 54/100\n",
      "10160/10160 [==============================] - 1s 142us/sample - loss: 1.7556 - mean_squared_error: 1.7556 - val_loss: 3.6935 - val_mean_squared_error: 3.6935\n",
      "Epoch 55/100\n",
      "10160/10160 [==============================] - 1s 141us/sample - loss: 1.7532 - mean_squared_error: 1.7532 - val_loss: 3.4954 - val_mean_squared_error: 3.4954\n",
      "Epoch 56/100\n",
      "10160/10160 [==============================] - 2s 217us/sample - loss: 1.7142 - mean_squared_error: 1.7142 - val_loss: 3.6168 - val_mean_squared_error: 3.6168\n",
      "Epoch 57/100\n",
      "10160/10160 [==============================] - 2s 161us/sample - loss: 1.7068 - mean_squared_error: 1.7068 - val_loss: 3.5337 - val_mean_squared_error: 3.5337\n",
      "Epoch 58/100\n",
      "10160/10160 [==============================] - 1s 144us/sample - loss: 1.6563 - mean_squared_error: 1.6563 - val_loss: 3.8599 - val_mean_squared_error: 3.8599\n",
      "Epoch 59/100\n",
      "10160/10160 [==============================] - 1s 145us/sample - loss: 1.6082 - mean_squared_error: 1.6082 - val_loss: 3.6901 - val_mean_squared_error: 3.6901\n",
      "Epoch 60/100\n",
      "10160/10160 [==============================] - 1s 141us/sample - loss: 1.6040 - mean_squared_error: 1.6040 - val_loss: 3.8137 - val_mean_squared_error: 3.8137\n",
      "Epoch 61/100\n",
      "10160/10160 [==============================] - 1s 145us/sample - loss: 1.5615 - mean_squared_error: 1.5615 - val_loss: 3.8989 - val_mean_squared_error: 3.8989\n",
      "Epoch 62/100\n",
      "10160/10160 [==============================] - 2s 187us/sample - loss: 1.5633 - mean_squared_error: 1.5633 - val_loss: 3.5625 - val_mean_squared_error: 3.5625\n",
      "Epoch 63/100\n",
      "10160/10160 [==============================] - 1s 145us/sample - loss: 1.5335 - mean_squared_error: 1.5335 - val_loss: 4.8048 - val_mean_squared_error: 4.8048\n",
      "Epoch 64/100\n",
      "10160/10160 [==============================] - 2s 206us/sample - loss: 1.5183 - mean_squared_error: 1.5183 - val_loss: 3.5855 - val_mean_squared_error: 3.5855\n",
      "Epoch 65/100\n",
      "10160/10160 [==============================] - 1s 146us/sample - loss: 1.4935 - mean_squared_error: 1.4935 - val_loss: 3.6871 - val_mean_squared_error: 3.6871\n",
      "Epoch 66/100\n",
      "10160/10160 [==============================] - 1s 144us/sample - loss: 1.4786 - mean_squared_error: 1.4786 - val_loss: 3.6565 - val_mean_squared_error: 3.6565\n",
      "Epoch 67/100\n",
      "10160/10160 [==============================] - 1s 147us/sample - loss: 1.4645 - mean_squared_error: 1.4645 - val_loss: 3.5957 - val_mean_squared_error: 3.5957\n",
      "Epoch 68/100\n",
      "10160/10160 [==============================] - 1s 146us/sample - loss: 1.4223 - mean_squared_error: 1.4223 - val_loss: 3.6747 - val_mean_squared_error: 3.6747\n",
      "Epoch 69/100\n",
      "10160/10160 [==============================] - 1s 147us/sample - loss: 1.4186 - mean_squared_error: 1.4186 - val_loss: 4.3573 - val_mean_squared_error: 4.3573\n",
      "Epoch 70/100\n",
      "10160/10160 [==============================] - 1s 142us/sample - loss: 1.3963 - mean_squared_error: 1.3963 - val_loss: 3.8841 - val_mean_squared_error: 3.8841\n",
      "Epoch 71/100\n",
      "10160/10160 [==============================] - 2s 148us/sample - loss: 1.3620 - mean_squared_error: 1.3620 - val_loss: 3.7469 - val_mean_squared_error: 3.7469\n",
      "Epoch 72/100\n",
      "10160/10160 [==============================] - 2s 149us/sample - loss: 1.3452 - mean_squared_error: 1.3452 - val_loss: 3.8689 - val_mean_squared_error: 3.8689\n",
      "Epoch 73/100\n",
      "10160/10160 [==============================] - 1s 146us/sample - loss: 1.3297 - mean_squared_error: 1.3297 - val_loss: 3.8036 - val_mean_squared_error: 3.8036\n",
      "Epoch 74/100\n",
      "10160/10160 [==============================] - 1s 147us/sample - loss: 1.3198 - mean_squared_error: 1.3198 - val_loss: 3.7612 - val_mean_squared_error: 3.7612\n",
      "Epoch 75/100\n",
      "10160/10160 [==============================] - 2s 151us/sample - loss: 1.2898 - mean_squared_error: 1.2898 - val_loss: 4.1661 - val_mean_squared_error: 4.1661\n",
      "Epoch 76/100\n",
      "10160/10160 [==============================] - 1s 144us/sample - loss: 1.2724 - mean_squared_error: 1.2724 - val_loss: 3.8477 - val_mean_squared_error: 3.8478\n",
      "Epoch 77/100\n",
      "10160/10160 [==============================] - 1s 144us/sample - loss: 1.2818 - mean_squared_error: 1.2818 - val_loss: 3.8950 - val_mean_squared_error: 3.8950\n",
      "Epoch 78/100\n",
      "10160/10160 [==============================] - 1s 147us/sample - loss: 1.2655 - mean_squared_error: 1.2655 - val_loss: 3.6793 - val_mean_squared_error: 3.6793\n",
      "Epoch 79/100\n",
      "10160/10160 [==============================] - 1s 147us/sample - loss: 1.2208 - mean_squared_error: 1.2208 - val_loss: 3.6861 - val_mean_squared_error: 3.6861\n",
      "Epoch 80/100\n",
      "10160/10160 [==============================] - 1s 146us/sample - loss: 1.2198 - mean_squared_error: 1.2198 - val_loss: 3.7397 - val_mean_squared_error: 3.7397\n",
      "Epoch 81/100\n",
      "10160/10160 [==============================] - 1s 145us/sample - loss: 1.2104 - mean_squared_error: 1.2104 - val_loss: 4.0781 - val_mean_squared_error: 4.0781\n",
      "Epoch 82/100\n",
      "10160/10160 [==============================] - 1s 146us/sample - loss: 1.1848 - mean_squared_error: 1.1848 - val_loss: 3.6163 - val_mean_squared_error: 3.6163\n",
      "Epoch 83/100\n",
      "10160/10160 [==============================] - 2s 148us/sample - loss: 1.1882 - mean_squared_error: 1.1882 - val_loss: 3.9425 - val_mean_squared_error: 3.9425\n",
      "Epoch 84/100\n",
      "10160/10160 [==============================] - 1s 147us/sample - loss: 1.1677 - mean_squared_error: 1.1677 - val_loss: 3.9757 - val_mean_squared_error: 3.9757\n",
      "Epoch 85/100\n",
      "10160/10160 [==============================] - 1s 145us/sample - loss: 1.1562 - mean_squared_error: 1.1562 - val_loss: 3.8740 - val_mean_squared_error: 3.8740\n",
      "Epoch 86/100\n",
      "10160/10160 [==============================] - 2s 149us/sample - loss: 1.1305 - mean_squared_error: 1.1305 - val_loss: 3.7367 - val_mean_squared_error: 3.7367\n",
      "Epoch 87/100\n",
      "10160/10160 [==============================] - 1s 145us/sample - loss: 1.1209 - mean_squared_error: 1.1209 - val_loss: 3.6688 - val_mean_squared_error: 3.6688\n",
      "Epoch 88/100\n",
      "10160/10160 [==============================] - 2s 148us/sample - loss: 1.0917 - mean_squared_error: 1.0917 - val_loss: 3.7631 - val_mean_squared_error: 3.7631\n",
      "Epoch 89/100\n",
      "10160/10160 [==============================] - 1s 146us/sample - loss: 1.0655 - mean_squared_error: 1.0655 - val_loss: 5.2865 - val_mean_squared_error: 5.2865\n",
      "Epoch 90/100\n",
      "10160/10160 [==============================] - 2s 150us/sample - loss: 1.0666 - mean_squared_error: 1.0666 - val_loss: 3.6659 - val_mean_squared_error: 3.6659\n",
      "Epoch 91/100\n",
      "10160/10160 [==============================] - 2s 149us/sample - loss: 1.0431 - mean_squared_error: 1.0431 - val_loss: 3.8795 - val_mean_squared_error: 3.8795\n",
      "Epoch 92/100\n",
      "10160/10160 [==============================] - 2s 151us/sample - loss: 1.0423 - mean_squared_error: 1.0423 - val_loss: 3.7042 - val_mean_squared_error: 3.7042\n",
      "Epoch 93/100\n",
      "10160/10160 [==============================] - 2s 149us/sample - loss: 1.0280 - mean_squared_error: 1.0280 - val_loss: 3.9558 - val_mean_squared_error: 3.9558\n",
      "Epoch 94/100\n",
      "10160/10160 [==============================] - 2s 149us/sample - loss: 1.0194 - mean_squared_error: 1.0194 - val_loss: 3.8744 - val_mean_squared_error: 3.8744\n",
      "Epoch 95/100\n",
      "10160/10160 [==============================] - 1s 146us/sample - loss: 0.9890 - mean_squared_error: 0.9890 - val_loss: 5.0686 - val_mean_squared_error: 5.0686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100\n",
      "10160/10160 [==============================] - 1s 140us/sample - loss: 1.0081 - mean_squared_error: 1.0081 - val_loss: 3.8272 - val_mean_squared_error: 3.8272\n",
      "Epoch 97/100\n",
      "10160/10160 [==============================] - 2s 203us/sample - loss: 0.9655 - mean_squared_error: 0.9655 - val_loss: 3.8837 - val_mean_squared_error: 3.8837\n",
      "Epoch 98/100\n",
      "10160/10160 [==============================] - 1s 142us/sample - loss: 0.9522 - mean_squared_error: 0.9522 - val_loss: 4.1617 - val_mean_squared_error: 4.1617\n",
      "Epoch 99/100\n",
      "10160/10160 [==============================] - 1s 145us/sample - loss: 0.9406 - mean_squared_error: 0.9406 - val_loss: 3.9506 - val_mean_squared_error: 3.9506\n",
      "Epoch 100/100\n",
      "10160/10160 [==============================] - 1s 141us/sample - loss: 0.9319 - mean_squared_error: 0.9319 - val_loss: 3.7775 - val_mean_squared_error: 3.7775\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data_arr, train_labels_arr, epochs=100, validation_split = 0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEKCAYAAAASByJ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3dd3gc1dnw4d+j3iWry5ZlucsVYwtsTDM2xXT4QiCEhPq+fiEJgSSEAGmQSgoJhJCAQw/ETiC0gMGU0IuNbdx7tyzJkiWrd+l8f5xZq1hlV9rVrqTnvq69dmd2dvaMZjXPnC7GGJRSSqkgfydAKaVUYNCAoJRSCtCAoJRSyqEBQSmlFKABQSmllEMDglJKKcAPAUFEHheRIhHZ2Gbd70Rkq4isF5EXRSShv9OllFJDnT9yCE8CCzusewuYaoyZDmwH7uzvRCml1FDX7wHBGPMBUNph3ZvGmCZn8TMgs7/TpZRSQ12IvxPQieuBf3b1pogsAhYBREdHz8rJyemvdCml1KCwevXqw8aYlI7rAyogiMgPgSbg2a62McYsBhYD5ObmmlWrVvVT6pRSanAQkX2drQ+YgCAi1wIXAAuMDrCklFL9LiACgogsBG4HTjfG1Pg7PUopNRT5o9npEuBTYKKI5InIDcCfgVjgLRFZKyIP93e6lFJqqOv3HIIx5spOVj/W3+lQSvlXY2MjeXl51NXV+Tspg1ZERASZmZmEhoa6tX1AFBkppYaevLw8YmNjyc7ORkT8nZxBxxhDSUkJeXl5jB492q3P6NAVSim/qKurIykpSYOBj4gISUlJHuXANCAopfxGg4Fvefr31YCglFIK0ICglBqiSkpKmDFjBjNmzCA9PZ0RI0YcXW5oaHBrH9dddx3btm1z+zsfffRRUlJSjn7PjBkzPPq8r2mlslJqSEpKSmLt2rUA3H333cTExHDbbbe128YYgzGGoKDO752feOIJj7/3qquu4v777+/y/aamJkJCWi/NPaWhrebmZoKDgz1Ok4vmEJRSqo2dO3cyefJkrrrqKqZMmUJBQQGLFi0iNzeXKVOm8LOf/ezotqeccgpr166lqamJhIQE7rjjDo477jhOOukkioqK3P7Ot99+m3nz5nHBBRcwbdq0TtPwzDPPMG3aNKZOncpdd90FcPR7b731VqZPn87KlSv7dOyaQ1BK+d09/9nE5vwKr+5z8vA4fnrhlF59duvWrTz99NPk5uYCcO+995KYmEhTUxNnnHEGl112GZMnT273mfLyck4//XTuvfdevvvd7/L4449zxx13HLPvZ599lvfee+/osusivmrVKjZv3kxWVhY7d+5sl4a8vDx+9KMfsWrVKuLj4znzzDN59dVXWbhwIeXl5Zx22mnd5jrcpTkEpZTqYOzYsUeDAcCSJUuYOXMmM2fOZMuWLWzevPmYz0RGRnLuuecCMGvWLPbu3dvpvq+66irWrl179BEWFgbASSedRFZWVqdpWLFiBfPnzyc5OZnQ0FC++tWv8sEHHwAQFhbGpZde6pXj1hyCUsrvensn7yvR0dFHX+/YsYMHHniAlStXkpCQwNe+9rVO2/a7LuwAwcHBNDU1HbONu9/Z2XJXIiMjvdZ8V3MISinVjYqKCmJjY4mLi6OgoIDly5f3expmz57Nu+++S0lJCU1NTSxdupTTTz/d69+jOQSllOrGzJkzmTx5Mjk5OYwaNYqTTz65T/vrWIfwyCOP9PiZzMxMfv7znzNv3jyMMVx44YWcf/75HudCeiIDeeoBnSBHqYFry5YtTJo0yd/JGPQ6+zuLyGpjTG7HbbXISCmlFKABQSmllEMDglJKKUADglJKKYcGBKWUUoAGBKWUUg4NCEqpIemMM844ppPZ/fffz0033dTt52JiYjpdHxwc3G5Y63vvvddrae0v2jFNKTUkXXnllSxdupRzzjnn6LqlS5fy29/+tlf7i4yMPDqcdlc6Dk/dcajrrri7XV9pDkEpNSRddtllvPbaa0cnw9m7dy/5+fmceuqpVFVVsWDBAmbOnMm0adN4+eWXe/092dnZ/OAHP2DmzJk899xzzJs3j1tvvZXc3FweeOAB9u7dy/z585k+fToLFixg//79AFx77bXceOONzJ49m9tvv90rx9wTzSEopfzv9TugcIN395k+Dc7tutgmMTGRE088kddff52LL76YpUuXcvnllyMiRERE8OKLLxIXF8fhw4eZM2cOF110UbeDyNXW1jJjxoyjy3feeSdXXHEFYCfjWbNmDQAPP/wwDQ0NuEZZuPDCC7nmmmu45pprePzxx/n2t7/NSy+9BEBeXh6ffPJJnya98YQGBKXUkOUqNnIFhMceewyws5TdddddfPDBBwQFBXHw4EEOHTpEenp6l/vqrsjIFRg6W/7000954YUXAPj617/eLjfw5S9/ud+CAWhAUEoFgm7u5H3p4osv5jvf+Q5r1qyhpqaGWbNmAXYAuuLiYlavXk1oaCjZ2dmdDnntrt4Obe3udt6idQhKqSErJiaGM844g+uvv54rr7zy6Pry8nJSU1MJDQ3l3XffZd++fT5Lw9y5c1m6dClgA9Gpp57qs+/qieYQlFJD2pVXXsmll1569KIMdlazCy+8kGnTppGbm0tOTk6P++lYh7Bw4UK3mp4++OCDXHfddfzud78jJSWFJ554oncH4gX9Pvy1iDwOXAAUGWOmOusSgX8C2cBe4HJjzJGe9qXDXys1cOnw1/0j0Ie/fhJY2GHdHcA7xpjxwDvOslJKqX7U7wHBGPMBUNph9cXAU87rp4BL+jVRSimlAqZSOc0YU+C8LgTS/JkYpVT/GMgzNg4Env59AyUgHGXsEXR5FCKySERWiciq4uLifkyZUsqbIiIiKCkp0aDgI8YYSkpKiIiIcPszgdLK6JCIZBhjCkQkAyjqakNjzGJgMdhK5f5KoFLKuzIzM8nLy0Nv7HwnIiKCzMxMt7cPlIDwCnANcK/z3PuBQ5RSA0JoaCijR4/2dzJUG/1eZCQiS4BPgYkikiciN2ADwVkisgM401lWSinVj/o9h2CMubKLtxb0a0KUUkq1E3CVykoppfxDA4JSSilAA4JSSimHW3UIzlhDPWkxxpT1MT1KKaX8xN1K5Xzn0fV0QRAMZPU5RUoppfzC3YCwxRhzfHcbiMgXXkiPUkopP3G3DuEkL22jlFIqQPUYEETkLOBBEZnhLC/qbDtjTO/nl1NKKeV37hQZXQ/cBPzIqVye0cP2SimlBiB3iowqjTFlxpjbgLOBE3ycJqWUUn7gTkB4zfXCGHMH8LTvkqOUUspfegwIxpiXOyw/6LvkKKWU8he3WhmJSKKIDPd1YpRSSvmPu81Of4+dpwAAEflERP4lIneIyAjfJE0ppVR/cjcgzKL9HAWxwGNAMnCntxOllFKq/7nbU7netJ/49L/GmOUi8iZ2shullFIDnLs5hDoRGeVaMMbc4jwbINQXCVNKKdW/3A0IvwReEpGctitFJIPAmZdZKaVUH7h1MXeKh+KAd0VkLbDReetLwA99lTillFL9x+27e2PMcyLyGnAeMAWoBS41xqzzVeKUUkr1H3cnyLkGuA9bxPQq8E1jTKUvE6aUUqp/uVuH8GPgLCAH2Af8ymcpUkop5RfuFhlVGGNcE+D8WERW+CpBSiml/MPdgJDhzIOwFdiCNjVVSqlBx92A8FNgGnCV8xwjIsuAdcB6Y8wSH6VPKaVUP3G32enitssikokNDNOxrY40ICil1ADnbiujBdicQDGAMSYPyANe92HalFJK9SN3i4zeAopEpAXbKW0DsN553mSMqfdR+pRSSvUTd5ud3gzkA38CfoGtXJ6F7Zuwz1uJEZHviMgmEdkoIktEJMJb+1ZKKdU9twKCMeYh4GTAAPcDjcAtxpgzjDHp3kiIM6/Ct4FcY8xUIBj4ijf2rZRSqmfu5hAwxtQaY34DnAGMA1aKyGwvpycEiBSRECAKmytRSinVD9ytVD4N20s5B5gEpAKVQJK3EmKMOSgivwf2Y8dJetMY82YnaVkELALIysry1tcrpdSQ524O4T3gRqAQuMkYM8sYM88Ys8xbCRGRYcDFwGhgOBAtIl/ruJ0xZrExJtcYk5uSkuKtr1dKqSHP3YBwE/AxcD6wQkQ2i8g/ReRHInKJl9JyJrDHGFNsjGkEXgDmemnfSimleuBux7RH2i536Jj2JeAlL6RlPzBHRKKwRUYLgFVe2K9SSik39Gq2M190TDPGrBCR54E1QBPwBbC4+08ppZTyFreKjERkjTe26Ykx5qfGmBxjzFRjzNe1w5tSSvUfd3MIk0RkfTfvCxDvhfQopZTyE3cDQo4b2zT3JSFKKaX8y91KZa8NT6GUUiowud1TWSml1ODmdkAQa6QvE6OUUsp/PBnLyABe65mslFIqsHhaZLRGRE7wSUqUUkr5lacd02YDV4nIPqAa29zUGGOmez1lSiml+pWnAeEcn6RCKaWU33lUZOQ0P00ALnQeCdokVSmlBgePAoKI3AI8i50PIRV4RkRu9kXCfKq5CWrL/J0KpZQKKJ4WGd0AzDbGVAOIyG+AT4EHvZ0wX8r//UnURKQx7pZX/Z0UpZQKGJ62MhLaD1HR7KwbUPJNMuEVe/2dDKWUCiie5hCewE6Q86KzfAnwmHeT5HvVMVmkHv4cWlogSDtrK6UUeNhTGXgOuA4odR7XGWPu91HafMYMG004jdSVHvB3UpRSKmC4nUMwxhgRWWaMmYadxGbACksZCzug5MAWRiSP8ndylFIqIAzJnsqxwycCUJm/088pUUqpwDEkeyqnZY6lwQTTdFgDglJKubgdEJw6hEXAgO+IlhwXxV5SCS4b8IeilFJe42kdwkNOHcKAFhQkFIcMZ3jNfn8nRSmlAsaQrEMAKI8cSVLDQTDG30lRSqmA0Js6hK+JyF4GcB0CQH3cKKKqaqH6MMSk+Ds5Sinld0N2tNOgpNGQDzWHdhKlAUEppdwrMhKR2+HoaKcnGmP2uR7A//kygb4SmTYBgPKDW/2cEqWUCgzu1iF8pc3rOzu8t9BLaelXiSPG0mKE2kJteqqUUuB+QJAuXne2PCBkpgwjnyRM6W5/J0UppQKCuwHBdPG6s+UBISk6jP2kE16pTU+VUgrcDwjHiUiFiFQC053XrmWv9UsQkQQReV5EtorIFhE5yVv77uS7KA0bQXytDnCnlFLgZisjY0ywrxPieAB4wxhzmYiEAVG+/LLq6Cxiy5ZDXTlExPvyq5RSKuAFzGQAIhIPnIYzv4IxpsEY49N5LlsSsu2L0j2++5L6Klg8D/av8N13KKWUFwRMQABGA8XAEyLyhYg8KiLRHTcSkUUiskpEVhUXF/fpC0NSxgJQfciHLY2KtkD+F7D5Zd99h1JKeUEgBYQQYCbwV2PM8die0Hd03MgYs9gYk2uMyU1J6VuHsrjhti9CVf62Pu2nW65WTHmf++47lFLKCwIpIOQBecYYV9nK89gA4TMZKUkUmQQai3f57kuOOMVRBeugqd5336OUUn3kUUAQ62si8hNnOUtETvRGQowxhcABEZnorFoAbPbGvrsyclgUe00aGftegfsmwR+nwSs3e3fAO1cOobkeCjd6b79KKeVlnuYQ/gKcBFzpLFcCD3kxPTcDz4rIemAG8Csv7vsYCVGhPMJlfJF0PoxbAKk5sOZp2PSi976kdDckjbevtdhIKRXAPB7t1BgzU0S+ADDGHHGah3qFMWYtkOut/fVERMgbNoeHY+fz6MW50NIMi0+Ht34CExZCmBdavZbuhkkXQmMN5K0Ebuz7PpVSygc8zSE0ikgwTu9kEUkBWryeqn40NjWaTfnltLQYCAqGhb+B8gPwyZ/6vvPaMqgpgcQxkJmrOQSlVEDzNCD8CXgRSBWRXwIf4eNiHV87a3IaBeV1fHHA6fKQfTJMuRQ+uh/K+tiL2VWhnDgGMk+Asv1Qeahv+/TE7vfhV5lQU9p/36mUGrDcDgjOnMofALcDvwYKgEuMMc/5KG394sxJaYSFBPHa+oLWlWf9HDDw9k/7tnNXhXLiGMh06t4PrurbPj1RsBYaKuHI3v77TqXUgOV2QDDGGGCZMWarMeYhY8yfjTFbfJi2fhEbEcrpE1JYtqHAFhsBJIyE3Oth00t9ayrq6gE9LBsypkNQaP8WG1UW2ufqvnXgU0oNDUN2TuW2LpieQWFFHav3H2ldmZkLphkOb+/9jkv3QEw6hEVDaCSkT4MD/RkQnFyPBgSllBs8DQizgU9FZJeIrBeRDU4T0QFtwaQ0wjsWG6VNtc+HNvV+x6W7bXGRS+YJkL8Gmpt6v09PaA5BKeUBTwPCOcBYYD5wIXCB8zygxYSHcMbEVJZtKKDZVWyUOBaCw+FQHzqTdRYQGmugyKf97VodzSEc7p/vU2owqyuHF28a1I00PAoIzhzKFUAaMKrNY8A7f3oGRZX1rNrrnOzgEEiZCId6efFuqIaqQkgc3bou0+li0R/1CMZoDkEpb9r3Kaz7B+z72N8p8RlPh674H2xLo+XAPc7z3d5PVv+bn5NKRGgQr21oW2w0pfdFRqVtmpy6DMu28y4Ubuh1Oo/K/wLWLun6/boyaKqzrzUgKNV3FXnOc0H32w1gnhYZ3QKcAOwzxpwBHA/4dM6C/hIdHsKCnDT+sy6fusZmuzJtir3Lry7xfIdtm5y6iNi6ib7US7h89Ec77lJDTefvu3IHCFRpQFCqzyry7XNlvn/T4UOeBoQ6Y0wdgIiEG2O2AhN7+MyA8fWTRnGkppEX1hy0K1In2+eiXlzAjwaE0e3Xp02xdQgtfezgfWgztDQ6w2F0wlV/kDROcwhKeUO5c13wRg7hkz/D6qf6vh8v8zQg5IlIAvAS8JaIvAzs836y/GP26ESmjojjsY922z4JfWlpdGQPRCUdOzVn2hRoqIKyPvzZGuug1Bmye98nnW/jyiGkT4Oaw30PQEoNdRVOQPBGDmHFI7D6ib7vx8s8rVS+1BhTZoy5G/gxdrrLi32RMH8QEf731DHsKq7mve1FEJMKUcm9CwgdWxi5eKM56+FtYJwL/N4uKrhcOYT0qdDSZOsUlFK9dzQgFHa/XU8a6+x4ab6cureXPK1U/onrAZyOHaL6Tp+kzE/Om5ZBRnwEf/tgj1PmP7mXAWFP5wEhJQeQvgWEIqeD+LgzbYulxrpjt6kstLmTBKcRmDtNT+vK4b4c2PJq79Om1GBkTGsdQl+LjMr2AcbepNUe6XHz/uRpkVF1m0czcC6Q7eU0+VVocBDXzs3m090lbDxYbu/oi7bYobHd1VgH5XmdB4TwGFuv0Jf+DYc2QXAYzLzaTryTv+bYbSoLIDYDopPtsjv1CLvft58bxM3qlOqVmlLbai92uB0frL6y9/sqaTNDY4DlEjwtMrqvzeOXwDygk6vewPaVE7OIDgvmsY/22IrlplrPBog7sgcwMGx05+/3pTkr2ACVPBGyT7XLnRUbVRZCbDpEO/NOuxMQdr1jnw/v6H3aVOCqKrIzAx7s5AZCdc9VXJQ5y1nuQy7B1eAEWkdEDhB9nVM5Csj0RkICSXxkKF+dncVLaw+yoibDrvTkjt7V8Wz4jM7fT5tqfxQN1a3rnr8BVv7Nvf0XbYbUSRCVCKlTYN9Hx25TWejkEFLtck8BwRjY+V/7uqSXAeGtn8KTF+joqoGqYJ2tEO2qIYLqmisgjHA6l/alYrl0N4RGO68HcEBwjV3kPDYB24D7fZM0//ruWROZOjyeb7xZjUE867G871Pbwih5Qufvp00BDBRttcsF62Dj8/DhfT0XTdWW2R9nmtMkNvtkOLASmhtbt2lpac0hRCUC0nNAKNkF5fttECnb7/kor0318PljsPdDePg02PIfzz6vfM918Qmwu9IB4WgOwRnbs085hF12ut7o1IA7F57mEFxjF10InA0MN8b82eupCgCRYcH87epcgsKjOSAZNOR70Lt4/yeQdZKtlO5M2hT77Mp1fPGsfa4sgF3/7X7frgplVx+JUSfb8ZHy17ZuU1tq+yjEZthZ4KKSeg4IruKi3BtsC6a22Vp37HrXlq2efx8kjYV/fg3e+41n+1C+5br4eHpule2DEBQCGcfZ5co+FhkljrF1iaV7vZI8b/F4LKM2j4PGmH4attM/0uMjWPz1WWxqHknp7jU0NLnRlr+iwBaZZJ3U9TYJ2TbLeGiTvbPe8C/IuQAiE+GLZ7rfv2tgvKMBYa59blts5Pqxxqbb5+iUngPCznfsj3TcArvsaT3CllcgPB6OvxqufwMmnmd7U/dmZNe8VbDsdpsbUt7jyiEEWDHFgFCRb2+wwmNs673eBoSm+tYGJ8NGD+wcgoh8t7uHrxLpT8dnDSNr6smkNxfw1ON/bp1Epyv7nfJZ14W6M0FBrc1Zt71um57lXgfTL4dty7ofTbFoM4THQbxTdROTaoum2pYLu9pJxzr1H9HJ3Tc7baq3RT1jF9iezQAlO7s/zraaG2HrazBxIYSEQUi4nYa0qdb2mXBXVRG89E14dAGsfATWLXX/s6pnrotP2f72RYyqZxUHIW6EfR07vLUJqqeO7LM58MSxNodQkd95s3E/8bTIKBe4CRjhPG4EZgKxzmNQmnLJ9zkUN5WvH/wZT/zzn9jJ47qw7xMIi4H06d3vNG2KLTL64hn7QxtzBsy4CpobYOO/u/5c0RZbody2OGrUXFtv4fon9zSHcGCFLXYaOx8i4uykPp4EhL0f2jbVky5qXTd8pn12t0VL0RZ4MBfW/xPmftv219j8kvtpUN1rabE518hEO/FT2X5/p2hgqTgI8a6AkN77HELbMc6GjQZM30Yt8DJPA0ImMNMY8z1jzPeAWUCWMeYeY8w93k9egAiLInXRS1RHpHHp1u+x5PV3u95236e24ik4pPt9pk21F9Gdb8FxV9qy/ozpkDat62IjY2yuInVS+/VjF9jy+/2f2WVXDiEmzT5Hp3Q/wN3Od2z56GinGWvyeM+KjDa/YovAXMVNYH/w4XF2VFZ3rFsCjdVw08dw9s9h6mWw/9Pe34mp9qoKbTv6sWfY5QArqghork5pccPtctzw3lcqu4accdUhQEAV4XkaENKAhjbLDc66QU9iUhj2v68QFhLMyZ/9H4vfWHVsTqH2iC3S6a64yMVVsQww46utr4+/CgrWdt5PobLQBpHUKe3Xj51vO6ptf8PZrsBWJIeE2+XoFKgv77rl0K53YOQcCHcyeUlj3W962tIMW1+F8WfZaUJdgoJsBZy7AWHH27beJcUZK3HKJfZ58yvufV51z3XRGbug/XJvvfsreGNQDVLQNVentDinmDY2A6oOedZZ1aV0t62DiEps7acUQMHZ04DwNLBSRO4WkXuAlcCTXk9VgApKHkvE1f9iRFApYz6+jbtf3tA6wxrA/hWA6b5C2cVVKZw1116AXaZdDkGhsPYfx37GNepqxxxCeIztpLbtdbvs6oPgEuPqnNZJPUJVsZ2fwXXnCJA03gY3d4b93v+ZLY6afNGx742YaYvFmhqOfa+t8jx7bOPPbl2XPN7moja92HMaVM9cF52sORAS2beWRsbYJsYrF/duaPiBxjUPwtEcQoYtduvNKMIlu2zuQMTW7YXFDNwcgtM7+TrgCFACXGuM+bUvEhaoQkbNIWjhLzkz+AsiPv8LNy9Z0zp/wv5P7MXcNTNadyIT4Iwfwpl3t18fnWTvtje/bP/x2urY5LStiefa7OjhHc6wFelt9tlNb2XX8NnZp7SuSx5vn93JJWx5xU412vZi7jL8eFsn0tPw4Tvess8d9zH5EjjwWeuww/1tzd/hifMGx0WvdA9IMCRkOc0d+3ARKt7mjKDbBJte8F4aA5Wr2DK+TaVy2/WeKN1tK5TBBoUAa2nkVkAQkRNEJB3AGLMGOynOmcB1IpLow/QFpKDZ/weTL+EHYf+ieON7fP2xFZTVNNj6gxEz2xeddOf02yFr9rHrcy6woyEWrG2/vmC9rReITjr2MxPOsc/bXm/tlOYS3U0OIe9zG8Rc7avB/ZZGDdWw/l8w4ezW4qa23K1Y3vEWxGe1Fhe5uIqNtvih2Gj1k/DKt+y4Tq9/v/+/39uO7LEt04JD7R1qX3IIribOMWn2/PdFk9OIojfNk/tLuSuH0KZSGTyvWG5qsP/Xbcc4S8zuOjjXHrH/8/3I3RzCIzh1ByJyGvBr4CmgHFjszQSJSLCIfCEigTvkpghc9CBBw0bxTNxDnHPwIR78072Y/C/cKy7qycRz7d1c21FH66tsk9SxCzr/TEKWLWLZtsyWb7YtMupugLu8VXaI7LZBLGGUDRI9VSx/8YztBHfSt7pOU2Ri9/UITfWw+z2bK+rYkc9fxUZrnob/3ALjzoLTvm8vWJsGeIun0j2tlZiJo22Lo97OkbH3I3uXPOcmm8PsS25j9ZPw/PWw6vHe78PXKvJtowvXMDBxvcwhlLmanLadVne0Xd+xPqKlBZZcCYvnwZ4Pep10T7kbEIKNMa7G8VcAi40x/zbG/BgY5+U03QJs8fI+vS8iDq54lvDU8Vwf+iY/rrsPaWlkR9RxPX+2J1GJtmJ6a5uAsPklO7HOrGu6/tyEhbZljmlxr8iopdlerF3d8V2CQ+yPtrscQnOjnfVp5BxbLt0ZEVts1F1A2PeJbV3UWZET2FzCgRWtd2m+tnUZvPJtO7T4Fc/A6XdAxgx47bsDeyrSI3tbKzGHjbaj5PZmPB5j7GCK2afAtC8DAhue612ajIHPnfG7Pvitvenpi2W3w2cP920fnak4aANgkHO5jE6xN2ye5hBcubK2dYaJo22xasfgsm6J/V8Oj4Hnru23ZsJuBwQRcbWjXAC0HV+hh/aV7hORTOB84FFv7dOn0ibDDcsJuusgBy57nR+F38H5y8JYstILJ2/ShVC8FQ47F+U1T9sOaCM7KWJymbCw9XXbHEJYDIREQHVR++2Lt9ogM6KTOo+emp5uesmOfXTyLd0fx4iZtu6jq7mfd7xl6yBcTV47muwUG21d1v33eMuqxyBhJFzxLIRG2OB46cN2uONXb+1dyxJ/qyu3ObmjOQTnDrU3xUaHd9jfUfYptggq+xTbd6S7vjld2fM+HN4OJ/yPvVn57C+e78PlwGMgeSkAACAASURBVErbmfGD33q/011Ffmv9Adgm4rHpnjc9LWnT5NSls5ZGNaXw1o/tzdb/vGOPZ+lVXf8PeZG7AWEJ8L4zZWYt8CGAiIzDFht5y/3A7UCXeVkRWSQiq0RkVXFxgNyxhYQxcupcvn/LbcwZm8qdL2zgrhc3uDfURVdyzrfPW/9jB8E7sMLOf9DV+EgAI2a15gba5hBEnM5pHeoQXKOydlYJnjTOXjA6K9s1Bj5+wHYeaxuEOjP8eNsio7CLsaB2vGkH6AuL7vz95PH2H2jH8u6/xxvqq2z2POdCGwxcUifB/B/bHNsjp9t5IwYSV5HOsI4BoRdFPXs/tM+uRgjTL7c5yc7m5OjJyr/Z5tFn/9LWm338p95X4L/3a3vXXlMCO99u/96RfXZcrSVfhT9MsR0gXS3y3FGe11pM5BKb0XMOwRj45EFbtNrUYP+fwuPsMbt01hfhnZ/ZYVvOv8/+/r/0qP3/eeVmn0+F61ZAcFoXfQ/bxPQU09oAPwi42RsJEZELgCJjzOoe0rLYGJNrjMlNSUnxxld7TXxUKE9cewI3zRvLP1bs58uPfErekV5G9fhMezHd8ip88Xdbpj/9K91/JigIxjuVy21zCNB5b+W8z20Zf2cT+SSNswPkddaLcuc7cGiD7VEc1MNPyFWx3FmxUelu25Kpq+Iil/HnwJ4P2w8X7gu7/muz7xPPPfa9uTfDZY/bu+2nL7J3bH0t4vCVAyvh3V9DY61ddt19ui4+8Zn299SbHMK+j+1vy/WbmXSRzeGt+2fn23eVcyg7YOu7jv+6Db4LfmKLDj+8z148934EH/7BDmfSk/0r7Lmb/0M75e26Ja3vNTfBs5fZgHF4uy3eDAqGJV+BZy/v+W9wtFPaiPbrO/ZWPrLv2JunDc/Bmz+Cl78Jfzre3tS4mpy6xGXa+gnXOdr9nq1XmXOTrdsD22BkwY/tiMjLbutdbsxNbjc7NcZ8Zox50RhT3WbddqfVkTecDFwkInuBpcB8EelhpLfAExwk/GBhDg9/bSa7i6o4/08f8d+th3q3s5wL4OAq2/wx57zW/gTdmfstW8kbk95+facBYbXNHXSW6zja9HSXM5z2IXtX9eaPbNFJ3AinDLkHcRk2LZ0FhA3P2+eeAsKEc2yZt68r17a9bjsNdVYnIgJTvwTf+txevLa+ai9e/W3dUvj7pZ3XZxz43L732Fnw/r3wuVPyejSHkG2fg4Jh2CjPmzsaYy/U2ae0/mYiE2DSBbaobeXfWi9W1SU2aP56JLx4o/1c2wuZa4L53Ovtc8pE20Fz5WL47Wh48nx45x67j576sbz3axsIZt9of5Ou8cHAFmcd3g6XPwU3r4LLHoMbP4Kzf2GD219P7r7RQk2J/e25xg5zadtbeesyeGA6/Ovq1uKqigJY9n3IPBG++pz9fNl+ZwrdNoJDbOOLza/AQ7Ph6Yvt/9a8O9pvd8p37Q3YqsfgjTt8FhS8Vv7fV8aYO3HmZxaRecBtxpiv+TVRfbBwagaTMuL4xrNruP7JVZw3LZ0bTx/L9MwE93cy6UL4789tL+OZV7v3mdRJcM4vj10fndJ+kp+6CluHMPX/db6fJCcg/PsGO85Ri3P3Exxm6xxO/74dyM4dw4+3uRFjWi8k1SU2Oz3x/PaVbJ0ZdbKtB9m+vPO7d29oabZ3cOPPtk0zuxIaAad+z7bF//QhmHWtvbiCDZzbltk784h4iBxmJ0nqbn+eqKuwF4PaIzaXcvUr9iahoRpe/4HNSUYlwVk/s3UzH91vL7hH9tjz37Zp8LDRnucQSnbaFmyjTm6//oI/2tzSstvs3B6TL4aXv2XrLSaeawc+XLfEXvjGzINRp8Dqp2xxo+tvB7ZfTuleSJlgW9PVlcPL34Dld8H5v+88Tfs/g93vwlk/t8WOx30FVvzVXuRnfM0GxowZ7cfZCg61Ob4p/89W2D53LRRutN/fMcfrmgehsyKj+nJ7vC/+n72Ib3sN/v0/8KXHbCu1pjq45K+QPM42zc5fe2xOAyB9mg0Io+bCub+z/5Mdm3GL2PPa0mTrWoJCbFDrrgi5FwImIAxGo5Ki+fdNc3no3Z08+clelm0o5ORxSdyxcBLTMuN73kHKRFuR3FBjB7/ri+hkm0NwXZTz1wDG1jt0un2SrTCuKrbZ49gMW4k+Ird9+bo7Ji6E7a/bO9YT/9eu+/A+W6G94Cc9fz4kzF5IdrzZPqj0xaHNtnPV6NPsct7n9m7Q3YCz4Kf2n/jtu+HLT9h0vXGHrdhsK3GMrX+YcqlNd0W+LXILCbcDICaPt3fs7vjsLzYYnP0L+O8vbVBY+Gt7J3p4B5zyHTj1NtsyZeRsePwc+zcv3XPsdK6JY5wWaR78Pfc6/Q+yOzQAiIiHK5fY4Sw+/L0NTMkT4Krn7PhcDTW2L8mml2DTy7aBBNjK5LbihsN1r7VfV7zF3jiMmAUzrmxdX19l6wo++L0NdifcYNdnHAcpk2xOqsUZxO/8P3Z+jPEj4NpX4bXv2XTnf2GbGWfNsdsXboDlP7Tbdvz7uQLEs1+2Aeb65bYl4Js/soG2cD2c82sbDFy6mkHxkr/C+X9obR7eFRE451c2KKz8m70ZceXkvSQgA4Ix5j3gPT8nwysiQoP53tkTWXTaGP6xYj9/+3APl/zlY755xji+dcY4wkJ6KLW77AnbjNTdi0ZXolNs+Xhduc3muyqUuwoIYO9IvOH4q+1d4vK77IUqMsE2N5zxVTtzlDsmnGOLaQ5tai1b7a3ibfDEQntRufolGxS2vW7vusad6d4+4kfAyd+G939jiyq2v2GDwZxv2qHM68ptU88P74Pnr4OP77cX38IOHY1CIm158YKfdH9hrim1zXxzLrB3t+nT4R9X2CKGmHS4+mUYc3rr9llz7BhXHz9gj2vMvPb7SxxjA3L1YXuj8MHvbO4vbYp9jJxtA4tLY609hzFpnefogoJtOffw422HylO+C2FR9r2wKHvnftxX7EW6cIMtfx87v+e/84K77Z31q7faG4LGWqivsP1nmuttjui837U2ShCx3/P2T506g5PaD7rYUUg4XPSgDSRv32N/Fyk5zmi7L9vf6nm/P/Y356qjqz5sf0MJI+15aayDd39hc1Gzb+z5+MCmvatGFR2JwLm/tTk/LwcDAOl2KOdj0iLhwJeAbNoEE2OMl64cnsnNzTWrVq3yx1f3WnlNI/f8ZxMvfHGQSRlx/PC8Scwdm0RQkHezfsdY9094cZFtxpaZC//4ih3q4luf+/Z7XapL4OGTITTK/nNtXw43rz62bLYrlYVw30R74Tz1e91vu/t9G/xGn9Y6wJ9LVbGdb8FVrFNTAoves3d6selwjQe9ohuq4cFZtnNdbSnMus4Wn7S9sLc023Lsj/5oK/AnLnQq/o3thbptmb17XnivDQwuB1ba4pkJ59py5rd+YlvhfOPT1rGs9n5sL1qn39753eWBz+ExJ8Cd/gM4467W97Yvh39cbicy2r7cXvxDo1v7JoRG2ZZuUy+zRY0rHraBY+7NNofSn6qK4blr7G8gLMqmbfhMW3cxcs6xIwtXFMAfJ9sbqWuX2VZs7miotp0QVz9pg9aJi+C022zRX0cV+XD/dHvTdNI32r+34y2bvs5GFAgQIrLaGHNM80JPA8Ib2Gamq4GjDbKNMX6oXRuYAcHlzU2F/PCljRRX1jMiIZIv52by1dlZpMZ6WBzjrpJd8PCptgnoqd+DFY/Yu+5L+tD221N7P4anLrD/qHO/bYe59sQjp9k76hu6aILa1GBzIa7OTuFx9hjHnGGDUMIo2+KkcCNc+5oNCH+bb/9xS3fDwt/AHDfv6lzWLoGXboTpV8AlD/fc6qqjlhb419ft3feVS+xd8zs/g0+dmWkTx9g7zbd+auuUvvQ3z/b/zJds0colD7cvcjm8A/6cC4jt7Ljgp7ZDZE2pLRff8gpsfMGOrgs253Tyre0rlAPZ89fbCt4r/t67z7tTlNZQ05oLGmC8FRA2GmP6mF/3noEcEADqGptZvqmQ51bl8dHOw8SEh/CdsyZwzUmjCAn2dCBaN5Tn2TJR18QzF/yxtZVHf/nkQVuheMOb9gLkiXd/ZYs2vr/r2M9WFNi7yAMr7F1s9qmw5T/ODHRt27aLvUhMutAubn/T3ilj4JZ1rS1x3GWMLSJJm9bzHBhdaaiBJ8+zRVmJY+wdee4Ntgjog9/bYiYJtrm5nirgO8pfa4dAuOY/7cuzjbEtekbkQmYXxYZNDbD3A1sk1ddiOhVQvBUQFgMPGmM8mHHedwZ6QGhrd3EV9/xnM+9vLyYnPZYfXzCZuWOTEF/cje1+z3aWOedXdgrO/tbbiuG81fDofNsCZd6dMPIEe0e7cjF89ld7R3jxn9u3nGppdir5Ntj6h+HH26KGtlb+zQ7Ad+lf+3ZcfVFZCI+eaYstLn7INjMG+7fa8ZbtE+LqrKhUH3krIGzGjl20B6gHBDDGmB7mi/SNwRQQAIwxLN90iJ/9ZxP55XUcNzKBm04fw9mT031fxzAQGGPL4j9+wBZlDJ9p76obq21Z+5l3u19JHYhqy2ygjHCjBZpSfeCtgDCqs/XGGL9MCjrYAoJLXWMz/16TxyPv72Z/aQ3JMeHMHpPI7NGJzJuQSlbSwCy39Jr6Ktt08Yu/29FQT/mObRKrlHKLVwKCs6NhwHjgaO2nMab/xmdtY7AGBJem5hbe2FTI25sPsWJPKQXldQQHCZfnZnLLggmkx/uoAlopNah5K4fwP9jhqTOBtcAc4FNjjBsNir1vsAeEtowx7C+t4YmP9/Lsin0EiXDetAyGRYURHR7MuNQYLjpuuG/qHJRSg0pXAcHTZhG3ACcAnxljzhCRHOBX3kig6p6IMCopmrsvmsINp4zmj29t56Odh6lpaKa6oQljYMWeUn5+8VSCtb5BKdULngaEOmNMnYggIuHGmK0iMrHnjylvGpkYxR+uaO0G39Ji+P2b2/jLe7sorqznwSuPJyK0jz2blVJDjqcBIU9EEoCXgLdE5Ajglwpl1SooSLh9YQ6pseHc8+pmLv3LJ8zPSSEnPY7pmfGMSnKzW7xSakjzuFL56AdFTgfigTeMMT2MT+sbQ6kOwV2vbyjgj29vZ1dxNc0t9tyePy2D286ZyOhkDQxKKe9VKgtwFTDGGPMzEckC0o0xK72XVPdpQOhafVMzO4uqWL7pEI9+uJuGphYuP2EkZ05K5fiRwxgW7ebQ1UqpQcdbAeGv2Okt5xtjJjlNUN80xpzQw0d9QgOCe4oq63jwnZ0s/Xw/jc32fI9JiWZBTirnTstgRmaCdnxTagjxVkBYY4yZKSJfGGOOd9atM8Yc58W0uk0DgmdqGppYn1fOmv1HWLG7lE92Haax2ZAWF052UjRJMWEkRodx5qQ0Tp+Qok1YlRqkvNXstFFEggHj7DQFm2NQA0BUWAhzxiQxZ0wS35gH5bWNvLPlEO9uK+ZQeR3bD1VxqLyOZz7bz/jUGG44ZTRnT0knUYuXlBoSPM0hXAVcAcwCngQuA35sjPmXT1LXA80heF9DUwuvrs/n0Q/3sLmgAoDh8RFMGRHPaRNSuHB6BglRGiCUGsi8OXRFDuCagugdY8xWL6SvVzQg+I4xhjX7j7B63xE2HqxgfV4Ze0tqCA0WFuSkMX9SKlOGxzE+NbbnWd+UUgGlT0VGItJxGilX4fI5IoIx5qKOn1EDm4gwa1Qis0bZeQeMMWwuqOCFNQd5ee1B3thUCEBosHD8yGEsnJrOwqnpDE+I9GeylVJ94FYOQUSKgQPAEmAFrQEBAGPM+z5JXQ80h+AfLS2GvSXVbMqvYOPBct7fXszWwkoARidHkxobTlpcBFmJUczKHsbMrGHER4b6OdVKKZc+FRk5FclnAVcC04HXgCXGmE3eTqgnNCAEjl3FVbyxsZDN+RUUVdZRVFlP3pFamlsMIpCTHsdp45M5ZXwyJ2Qn6tAaSvmRN+sQwrGB4XfAPcaYP3sniZ7TgBDYquubWHegjM/3HuHT3YdZve8Ijc2G6LBgzpuWwZdzR3JC9jBt3qpUP+tzQHACwfnYYJANvAI8bow56MV0ekQDwsBSXd/Eyj2lvL6xgNfWF1Dd0ExaXDipsRHERoQwLCqMycPt+EvTRyQQH6XFTEr5Ql+LjJ4GpgLLgKXGmI3eT6LnNCAMXNX1Tby+sZAPdxRTUdtIZV0TxVX17CupObrNiIRIJqTFMCE9lqnD45meGU9WYpTmKJTqo74GhBag2lls+wHXnMpxXkmlhzQgDD7lNY1sOFjO+oNlbC+sZNuhKnYVVdHQbPs/xkeGsmBSKpfNzGTOmCQdckOpXuhTs1NjjDY0V/0iPiqUU5zKZ5eGpha2H6pk48FyVu07wvKNhbyw5iDD4yM4YXQi41JiGJcaw+ThcZqDUKoPej38dSDQHMLQVNfYzFubD/HKunw251dwsKz26HtxESFMz0wgJz2WsakxjE2JITspiuSYcM1NKOXw1lhGPiMiI4GngTRssdRiY8wD/k2VCkQRocFceNxwLjxuOGDrI3YVV7Epv4L1eeVsPFjOMyv2UdfYOsxWWEgQmcMimTI8nnOnpjNvYgpRYQHz81cqIARMDkFEMoAMY8waEYkFVgOXGGM2d/UZzSGorrS0GA6W1bKzuIoDpTXkHanlQGkNK/eUUlLdQERoECePTSY3O5FZo4YxPTNe+0aoISPgcwjGmAKgwHldKSJbgBFAlwFBqa4EBQkjE6MYmRjVbn1Tcwsr95by+oZCPt55mHe2FgEgAhlxEWQnR5OVGEVqbDgpcRGMSIhg2ogEUmLD/XEYSvWrgAkIbYlINnA8dpgMpbwmJDiIuWOTmTvWVlqXVNWzZn8Zm/Mr2FdSzZ6Sat7eUkRJdT1tM88jEiKZMTKBSRmx5KTHMWl4HMPjI7QCWw0qAVNk5CIiMcD7wC+NMS908v4iYBFAVlbWrH379vVzCtVQ0NTcQkl1A/tKalh3oIy1B8pYl1dG3pHWCuwRCZHMHpPInDFJzM9JJTlGcxFqYPDa0BW+JCKhwKvAcmPMH3raXusQVH+rrGt0msBWsGJPCZ/tLqW0uoEggdzsRBZOSWdCWizp8XaAv9gI7W2tAk/ABwSxee+ngFJjzK3ufEYDgvI3YwxbCipZvqmQNzYWsu1QZbv3Y8JDGJ4QQUZ8JJnDIsly6jXGpEQzOjma8BCtyFb9byAEhFOAD4ENtE7LeZcxZllXn9GAoALNwbJa8kprKKyoo6C8jsLyOvLLaskvryXvSC1lNY1Htw0OEkYlRZEcHU5wkBASLKTFRTBr1DByRw1jbEqM9p1QPjEQWhl9RId5FpQaaEYkRDKim0mCKuoa2V9Sw67iKnYWVbH9UCXltY00tbRQ12TYtLWI51fnAbblU2RoMFFhNpcxb2IqC3JSmTYiXgOF8omAySH0huYQ1GBjjGHP4WpW7TtCXmkNNQ3N1DQ2s72wkjX7j9BiIDkmnNMmJDNvYionZicSGRZMWHAQYSFBBGugUG4I+ByCUspOXTomJYYxKTHHvHekuoH3thfx7tZi/ru1iBfWtB95Piw4iBlZCcwZk8TxWQmEBwfRbAxBImQnR2szWdUjzSEoNQA1txjW5ZWx6WA59U0tNDYbDlfV8/neUjYeLKelk3/r2IgQpwVUBCkx4STHhDEyMYrRydFkJ0cTpy2ihgzNISg1iAQHCTOz7HzVHZXXNrK1oALjbNfY3MKu4mq2FVaw41AVW/Ir+KCqnsq6pnafiw4LJskJFInRrucwJqTFMntMIhnxXdeNqMFBA4JSg0x8ZCizxyS1W+fqmd1WXWMz+0tr2HO4mr2HqymqrOdwlX3kHalhfV4ZpdUNNDnZjVFJNjcREhREaLCQEBVK5rAospxmtBPSYgkN1pHyBzINCEoNURGhwUxIi2VCWmyX2zS3GLYWVvDZ7lI+211CUUUdjc2GhuYWjlQ3UFLdcHTbsJAgJmfEkZMeS2pcBKmx4aTGhpMcG+4UUYUTERqk9RgBTOsQlFK9Vl3fxIEjNWw/VMWGvDI2HCxnZ1H1MWNBuYQEydH5s8elxpCTHsuE9FiGJ0SSFhdBUnQY9U0tVNU3UVPfRHxkKInRYYRozsOrtA5BKeV10eEh5KTHkZMex0XO/BRgx4I6XNVAUWWdLYaqbOBwta23qKxrpKSqgW2HKnl7y6FOK8DbChJIignnuMx45o5N5uRxyaTFhSMiBInN6WhRlXdoQFBKeV1IcBDp8RGkx0d0u11dYzN7DldTWFFHUUUdh6saCA8JIjYihIjQYCrqmiiuqCO/vI5Ve0t5e0tRp/txfSY1NoIxKdGMSYlhWFQojc22BVZEaDAjEuzwIWlxEcRHhhIWokGkIw0ISim/iQgNZlJGHJMy4tzaPu9IDZ/tLqWithGD7chX19hMZV0TFXVN5JfVsj6vnGUbCnrMeUSFBZMcE87o5GjGpEQzclgUcZGh7Yq0EqPD+n6QA4gGBKXUgJE5LIrLZkX1uF1dYzO1Dc2EhgQREiTUNDRz8EgtB8tqOFRRT3ltI+W1jRyqqGPP4Wo+31tKTUPzMftJjglnQpqdm3tsSjSjkqMJDQqixRg7qVJ8BJnDogbNbHsaEJRSg05EaHC7i3REaDCJ0WFMy4zvdHtjDGU1jbaOo76Rw1UN7DhUyfZDlWw7VMVLXxyksr6p08+KQFpsBJFhwYizHBcZSlJ0OEnRYcRFhhAdHkJMeAgjEiLJyYgjKzGK4CChqr6JgrJaIsOCyRzWc6DzNQ0ISqkhT0QYFh3GsDZFRKdPSDn62hhDcVU9B0praDG2oru5BfLLatlXUsP+0hoamlswxtBiDBW1TRwsq2V9XhmVdU3UNrbPfUSGBhMSJO2CzIiESOaMSWJ6ZjyJ0bZTYEJUKAlRYcRHhhIdFuzzJrsaEJRSqgciQmpsBKmx3VeSd6W5xVBV38T+khq2FFawrbCSpuYWMhIiyYiPoKymkc92l/DfrYf495q8TvcRHCREhwUTE25zHEsXzSHJy7P0aUBQSikfCw4S4iNDmZYZ32Wx1TVzs2lpMZTWNHCkuoHS6gaO1DQcre8or22kur6ZqvomquubiAzzfr2FBgSllAoQQUFCstOr2y/f75dvVUopFXA0ICillAI0ICillHJoQFBKKQVoQFBKKeXQgKCUUgrQgKCUUsqhAUEppRSgAUEppZRDA4JSSilAA4JSSimHBgSllFJAgAUEEVkoIttEZKeI3OHv9Cil1FASMAFBRIKBh4BzgcnAlSIy2b+pUkqpoSNgAgJwIrDTGLPbGNMALAUu9nOalFJqyAik+RBGAAfaLOcBsztuJCKLgEXOYpWIbOvl9yUDh3v52YFqKB4zDM3jHorHDEPzuHtzzKM6WxlIAcEtxpjFwOK+7kdEVhljcr2QpAFjKB4zDM3jHorHDEPzuL15zIFUZHQQGNlmOdNZp5RSqh8EUkD4HBgvIqNFJAz4CvCKn9OklFJDRsAUGRljmkTkW8ByIBh43BizyYdf2edipwFoKB4zDM3jHorHDEPzuL12zGKM8da+lFJKDWCBVGSklFLKjzQgKKWUAoZgQBgqw2OIyEgReVdENovIJhG5xVmfKCJvicgO53mYv9PqbSISLCJfiMirzvJoEVnhnPN/Oo0WBhURSRCR50Vkq4hsEZGTBvu5FpHvOL/tjSKyREQiBuO5FpHHRaRIRDa2WdfpuRXrT87xrxeRmZ5815AKCENseIwm4HvGmMnAHOCbzrHeAbxjjBkPvOMsDza3AFvaLP8G+KMxZhxwBLjBL6nyrQeAN4wxOcBx2OMftOdaREYA3wZyjTFTsQ1RvsLgPNdPAgs7rOvq3J4LjHcei4C/evJFQyogMISGxzDGFBhj1jivK7EXiBHY433K2ewp4BL/pNA3RCQTOB941FkWYD7wvLPJYDzmeOA04DEAY0yDMaaMQX6usa0kI0UkBIgCChiE59oY8wFQ2mF1V+f2YuBpY30GJIhIhrvfNdQCQmfDY4zwU1r6jYhkA8cDK4A0Y0yB81YhkOanZPnK/cDtQIuznASUGWOanOXBeM5HA8XAE05R2aMiEs0gPtfGmIPA74H92EBQDqxm8J9rl67ObZ+ucUMtIAw5IhID/Bu41RhT0fY9Y9scD5p2xyJyAVBkjFnt77T0sxBgJvBXY8zxQDUdiocG4bkehr0bHg0MB6I5tlhlSPDmuR1qAWFIDY8hIqHYYPCsMeYFZ/UhVxbSeS7yV/p84GTgIhHZiy0OnI8tW09wihVgcJ7zPCDPGLPCWX4eGyAG87k+E9hjjCk2xjQCL2DP/2A/1y5dnds+XeOGWkAYMsNjOGXnjwFbjDF/aPPWK8A1zutrgJf7O22+Yoy50xiTaYzJxp7b/xpjrgLeBS5zNhtUxwxgjCkEDojIRGfVAmAzg/hcY4uK5ohIlPNbdx3zoD7XbXR1bl8BrnZaG80BytsULfXMGDOkHsB5wHZgF/BDf6fHh8d5CjYbuR5Y6zzOw5apvwPsAN4GEv2dVh8d/zzgVef1GGAlsBN4Dgj3d/p8cLwzgFXO+X4JGDbYzzVwD7AV2Aj8HQgfjOcaWIKtJ2nE5gZv6OrcAoJtSbkL2IBtheX2d+nQFUoppYChV2SklFKqCxoQlFJKARoQlFJKOTQgKKWUAjQgKKWUcmhAUKobItIsImvbPLw2QJyIZLcdwVIpfwuYKTSVClC1xpgZ/k6EUv1BcwhK9YKI7BWR34rIBhFZKSLjnPXZIvJfZyz6d0Qky1mfJiIvisg65zHX2VWwiPzNGdf/TRGJ9NtBqSFPA4JS3YvsUGR0RZv3yo0x04A/Y0dZBXgQeMoYMx14FviTs/5PwPvGmOOw4wxtctaPBx4yxkwByoAv+fh4lOqS9lRWqhsiUmWMielk/V5gvjFmtzOIYKExJklEDgMZxphGZ32BMSZZRIqBTGNMj1E97wAAAMdJREFUfZt9ZANvGTvJCSLyAyDUGPML3x+ZUsfSHIJSvWe6eO2J+javm9F6PeVHGhCU6r0r2jx/6rz+BDvSKsBVwIfO63eAm+DonM/x/ZVIpdyldyNKdS9SRNa2WX7DGONqejpMRNZj7/KvdNbdjJ257PvYWcyuc9bfAiwWkRuwOYGbsCNYKhUwtA5BqV5w6hByjTGH/Z0WpbxFi4yUUkoBmkNQSinl0ByCUkopQAOCUkophwYEpZRSgAYEpZRSDg0ISimlAPj/af5UEHj4v1QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_history(history):\n",
    "  hist = pd.DataFrame(history.history)\n",
    "  hist['epoch'] = history.epoch\n",
    "  \n",
    "#   plt.figure()\n",
    "#   plt.xlabel('Epoch')\n",
    "#   plt.ylabel('Mean Abs Error [MPG]')\n",
    "#   plt.plot(hist['epoch'], hist['mean_absolute_error'],\n",
    "#            label='Train Error')\n",
    "#   plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n",
    "#            label = 'Val Error')\n",
    "#   plt.ylim([0,10])\n",
    "#   plt.legend()\n",
    "  \n",
    "  plt.figure()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Mean Square Error [$MPG^2$]')\n",
    "  plt.plot(hist['epoch'], hist['mean_squared_error'],\n",
    "           label='Train Error')\n",
    "  plt.plot(hist['epoch'], hist['val_mean_squared_error'],\n",
    "           label = 'Val Error')\n",
    "  plt.ylim([0,12])\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
