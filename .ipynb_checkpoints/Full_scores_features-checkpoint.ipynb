{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec\n",
    "import spacy; from spacy.lang.en import English; nlp = English()\n",
    "from metrics import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = pd.read_pickle(\"./fullfeatures.pickle\")\n",
    "outputs = pd.read_csv(\"./essay_scores.csv\", sep=\",\")\n",
    "# outputs.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>tokenize</th>\n",
       "      <th>sequence</th>\n",
       "      <th>pad_seq</th>\n",
       "      <th>len</th>\n",
       "      <th>sent_count</th>\n",
       "      <th>avg_sent_len</th>\n",
       "      <th>punc_count</th>\n",
       "      <th>oov_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adv_count</th>\n",
       "      <th>adj_count</th>\n",
       "      <th>det_count</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>avg_depth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[dear, local, newspaper, i, think, effects, co...</td>\n",
       "      <td>[302, 485, 427, 7, 67, 524, 42, 20, 18, 21, 19...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>556.818182</td>\n",
       "      <td>1.171429</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>2.742857</td>\n",
       "      <td>2.028571</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>12</td>\n",
       "      <td>3.987212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[dear, @caps, @caps, i, believe, that, using, ...</td>\n",
       "      <td>[302, 11, 11, 7, 176, 6, 312, 42, 60, 634, 105...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1.208571</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>779.210526</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>13</td>\n",
       "      <td>4.482301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[dear, @caps, @caps, @caps, more, and, more, p...</td>\n",
       "      <td>[302, 11, 11, 11, 76, 3, 76, 21, 122, 42, 35, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0.808571</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>495.250000</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>1.685714</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>10</td>\n",
       "      <td>3.531148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[dear, local, newspaper, @caps, i, have, found...</td>\n",
       "      <td>[302, 485, 427, 11, 7, 20, 250, 6, 73, 1267, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1.511429</td>\n",
       "      <td>1.257143</td>\n",
       "      <td>420.795455</td>\n",
       "      <td>1.257143</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>3.228571</td>\n",
       "      <td>1.085714</td>\n",
       "      <td>1.085714</td>\n",
       "      <td>1.514286</td>\n",
       "      <td>10</td>\n",
       "      <td>3.268761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>[dear, @location, i, know, having, computers, ...</td>\n",
       "      <td>[302, 127, 7, 90, 235, 42, 95, 4, 346, 288, 18...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1.351429</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>551.833333</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>0.371429</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>3.257143</td>\n",
       "      <td>1.314286</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>1.742857</td>\n",
       "      <td>13</td>\n",
       "      <td>3.559846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                           tokenize  \\\n",
       "0         1          1  [dear, local, newspaper, i, think, effects, co...   \n",
       "1         2          1  [dear, @caps, @caps, i, believe, that, using, ...   \n",
       "2         3          1  [dear, @caps, @caps, @caps, more, and, more, p...   \n",
       "3         4          1  [dear, local, newspaper, @caps, i, have, found...   \n",
       "4         5          1  [dear, @location, i, know, having, computers, ...   \n",
       "\n",
       "                                            sequence  \\\n",
       "0  [302, 485, 427, 7, 67, 524, 42, 20, 18, 21, 19...   \n",
       "1  [302, 11, 11, 7, 176, 6, 312, 42, 60, 634, 105...   \n",
       "2  [302, 11, 11, 11, 76, 3, 76, 21, 122, 42, 35, ...   \n",
       "3  [302, 485, 427, 11, 7, 20, 250, 6, 73, 1267, 1...   \n",
       "4  [302, 127, 7, 90, 235, 42, 95, 4, 346, 288, 18...   \n",
       "\n",
       "                                             pad_seq       len  sent_count  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  1.000000    0.628571   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  1.208571    0.542857   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  0.808571    0.571429   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  1.511429    1.257143   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  1.351429    0.857143   \n",
       "\n",
       "   avg_sent_len  punc_count  oov_count  noun_count  verb_count  adv_count  \\\n",
       "0    556.818182    1.171429   0.228571    2.742857    2.028571   0.742857   \n",
       "1    779.210526    0.828571   0.428571    3.400000    2.857143   0.885714   \n",
       "2    495.250000    0.628571   0.200000    2.400000    1.685714   0.514286   \n",
       "3    420.795455    1.257143   1.400000    4.600000    3.228571   1.085714   \n",
       "4    551.833333    1.285714   0.371429    3.400000    3.257143   1.314286   \n",
       "\n",
       "   adj_count  det_count  max_depth  avg_depth  \n",
       "0   0.514286   0.914286         12   3.987212  \n",
       "1   0.600000   1.285714         13   4.482301  \n",
       "2   0.542857   0.857143         10   3.531148  \n",
       "3   1.085714   1.514286         10   3.268761  \n",
       "4   0.828571   1.742857         13   3.559846  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>score</th>\n",
       "      <th>Prompt Adherence</th>\n",
       "      <th>Narrativity</th>\n",
       "      <th>Language</th>\n",
       "      <th>Word Choice</th>\n",
       "      <th>Sentence Fluency</th>\n",
       "      <th>Organization</th>\n",
       "      <th>Conventions</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>12701.000000</td>\n",
       "      <td>12701.000000</td>\n",
       "      <td>12701.000000</td>\n",
       "      <td>7065.000000</td>\n",
       "      <td>7065.000000</td>\n",
       "      <td>8398.000000</td>\n",
       "      <td>4303.000000</td>\n",
       "      <td>4303.000000</td>\n",
       "      <td>5636.000000</td>\n",
       "      <td>5636.000000</td>\n",
       "      <td>12701.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6350.750492</td>\n",
       "      <td>10146.244233</td>\n",
       "      <td>0.597175</td>\n",
       "      <td>0.451699</td>\n",
       "      <td>0.470193</td>\n",
       "      <td>0.509040</td>\n",
       "      <td>0.494585</td>\n",
       "      <td>0.515547</td>\n",
       "      <td>0.532032</td>\n",
       "      <td>0.546227</td>\n",
       "      <td>0.485648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3666.931576</td>\n",
       "      <td>6266.039109</td>\n",
       "      <td>0.233834</td>\n",
       "      <td>0.286019</td>\n",
       "      <td>0.277591</td>\n",
       "      <td>0.270596</td>\n",
       "      <td>0.207677</td>\n",
       "      <td>0.197891</td>\n",
       "      <td>0.231308</td>\n",
       "      <td>0.231587</td>\n",
       "      <td>0.269731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3176.000000</td>\n",
       "      <td>4372.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6351.000000</td>\n",
       "      <td>9941.000000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9526.000000</td>\n",
       "      <td>15513.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12701.000000</td>\n",
       "      <td>21633.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0      essay_id         score  Prompt Adherence  \\\n",
       "count  12701.000000  12701.000000  12701.000000       7065.000000   \n",
       "mean    6350.750492  10146.244233      0.597175          0.451699   \n",
       "std     3666.931576   6266.039109      0.233834          0.286019   \n",
       "min        0.000000      1.000000      0.000000          0.000000   \n",
       "25%     3176.000000   4372.000000      0.500000          0.250000   \n",
       "50%     6351.000000   9941.000000      0.625000          0.500000   \n",
       "75%     9526.000000  15513.000000      0.750000          0.666667   \n",
       "max    12701.000000  21633.000000      1.000000          1.000000   \n",
       "\n",
       "       Narrativity     Language  Word Choice  Sentence Fluency  Organization  \\\n",
       "count  7065.000000  8398.000000  4303.000000       4303.000000   5636.000000   \n",
       "mean      0.470193     0.509040     0.494585          0.515547      0.532032   \n",
       "std       0.277591     0.270596     0.207677          0.197891      0.231308   \n",
       "min       0.000000     0.000000     0.000000          0.000000      0.000000   \n",
       "25%       0.333333     0.333333     0.400000          0.400000      0.400000   \n",
       "50%       0.500000     0.500000     0.600000          0.600000      0.600000   \n",
       "75%       0.666667     0.666667     0.600000          0.600000      0.666667   \n",
       "max       1.000000     1.000000     1.000000          1.000000      1.000000   \n",
       "\n",
       "       Conventions       Content  \n",
       "count  5636.000000  12701.000000  \n",
       "mean      0.546227      0.485648  \n",
       "std       0.231587      0.269731  \n",
       "min       0.000000      0.000000  \n",
       "25%       0.400000      0.333333  \n",
       "50%       0.600000      0.500000  \n",
       "75%       0.666667      0.666667  \n",
       "max       1.000000      1.000000  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pranav_pro/SchoolSystem/.venv/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "w2vModel = Word2Vec.load('Essayw2v.model')\n",
    "vocab = set(w2vModel.wv.vocab)\n",
    "dic = w2vModel.wv\n",
    "w2v = dict(zip(w2vModel.wv.index2word, w2vModel.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "word_index = tokenizer.word_index\n",
    "MaxSequence = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in word_index.items():\n",
    "    if word in vocab:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = dic[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(len(word_index) + 1,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MaxSequence,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def buildData(x,y):  \n",
    "    # split the data into a training set and a validation set\n",
    "    indices = np.arange(x.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    x = x[indices]\n",
    "    y = y[indices]\n",
    "    nb_validation_samples = int(0.97 * x.shape[0])\n",
    "\n",
    "    trainX = x[:-nb_validation_samples]\n",
    "    trainY = y[:-nb_validation_samples]\n",
    "    testX = x[-nb_validation_samples:]\n",
    "    testY = y[-nb_validation_samples:]\n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, 1200)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1200, 300)    12767400    main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 1196, 64)     96064       embedding_2[27][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 34, 64)       0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_26 (Flatten)            (None, 2176)         0           max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "aux_input (InputLayer)          (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 2187)         0           flatten_26[0][0]                 \n",
      "                                                                 aux_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_50 (Dense)                (None, 64)           140032      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 1)            65          dense_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "aux_output (Dense)              (None, 1)            2177        flatten_26[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 13,005,738\n",
      "Trainable params: 238,338\n",
      "Non-trainable params: 12,767,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    main_input = layers.Input(shape=(MaxSequence,), dtype='int32', name='main_input')\n",
    "    emb = embedding_layer(main_input)\n",
    "    conv_out = layers.Conv1D(64, 5, activation='relu')(emb)\n",
    "    pooled = layers.MaxPooling1D(35)(conv_out)\n",
    "    flat = layers.Flatten()(pooled)\n",
    "    auxiliary_output = layers.Dense(1,activation=\"sigmoid\", name=\"aux_output\")(flat)\n",
    "    auxiliary_input = layers.Input(shape=(11,), name='aux_input')\n",
    "    x = layers.concatenate([flat, auxiliary_input])\n",
    "    dropped = layers.Dropout(0.4)(x) \n",
    "    d1 = layers.Dense(64, activation='relu')(x)\n",
    "    main_output = layers.Dense(1,activation=\"sigmoid\", name='main_output')(d1)\n",
    "    model = Model(inputs=[main_input, auxiliary_input], outputs=[main_output, auxiliary_output])\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                optimizer='adam')\n",
    "    return model\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model():\n",
    "#     model = Sequential()\n",
    "#     model.add(layers.InputLayer((MaxSequence,), dtype='int32'))\n",
    "#     model.add(embedding_layer)\n",
    "#     model.add(layers.Conv1D(64, 5, activation='relu'))\n",
    "#     model.add(layers.MaxPooling1D(35))\n",
    "#     model.add(layers.Flatten())\n",
    "#     model.add(layers.Dropout(0.4))\n",
    "#     model.add(layers.Dense(64, activation='relu'))\n",
    "#     model.add(layers.Dense(1,activation=\"linear\"))\n",
    "\n",
    "#     optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "#     model.compile(loss='mean_squared_error',\n",
    "#                 optimizer='adam',\n",
    "#                 metrics=['mean_squared_error'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class IntervalEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=10):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = kappa(self.y_val, y_pred*10,weights='quadratic')\n",
    "            print(\"interval evaluation - epoch: {:d} - score: {:.6f}\".format(epoch, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, 1200)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1200, 300)    12767400    main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 1196, 64)     96064       embedding_2[28][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling1D) (None, 34, 64)       0           conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_27 (Flatten)            (None, 2176)         0           max_pooling1d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "aux_input (InputLayer)          (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 2187)         0           flatten_27[0][0]                 \n",
      "                                                                 aux_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_51 (Dense)                (None, 64)           140032      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 1)            65          dense_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "aux_output (Dense)              (None, 1)            2177        flatten_27[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 13,005,738\n",
      "Trainable params: 238,338\n",
      "Non-trainable params: 12,767,400\n",
      "__________________________________________________________________________________________________\n",
      "Train on 571 samples, validate on 64 samples\n",
      "Epoch 1/10\n",
      "571/571 [==============================] - 8s 13ms/sample - loss: 0.2685 - main_output_loss: 0.2104 - aux_output_loss: 0.0574 - val_loss: 0.3131 - val_main_output_loss: 0.2655 - val_aux_output_loss: 0.0477\n",
      "Epoch 2/10\n",
      "571/571 [==============================] - 7s 12ms/sample - loss: 0.2302 - main_output_loss: 0.1985 - aux_output_loss: 0.0311 - val_loss: 0.2180 - val_main_output_loss: 0.1647 - val_aux_output_loss: 0.0533\n",
      "Epoch 3/10\n",
      "571/571 [==============================] - 7s 13ms/sample - loss: 0.1646 - main_output_loss: 0.1401 - aux_output_loss: 0.0246 - val_loss: 0.1724 - val_main_output_loss: 0.1263 - val_aux_output_loss: 0.0461\n",
      "Epoch 4/10\n",
      "571/571 [==============================] - 7s 12ms/sample - loss: 0.1636 - main_output_loss: 0.1376 - aux_output_loss: 0.0259 - val_loss: 0.1720 - val_main_output_loss: 0.1176 - val_aux_output_loss: 0.0544\n",
      "Epoch 5/10\n",
      "571/571 [==============================] - 7s 12ms/sample - loss: 0.1459 - main_output_loss: 0.1184 - aux_output_loss: 0.0280 - val_loss: 0.1572 - val_main_output_loss: 0.1129 - val_aux_output_loss: 0.0442\n",
      "Epoch 6/10\n",
      "571/571 [==============================] - 7s 12ms/sample - loss: 0.1271 - main_output_loss: 0.1044 - aux_output_loss: 0.0224 - val_loss: 0.1317 - val_main_output_loss: 0.0800 - val_aux_output_loss: 0.0517\n",
      "Epoch 7/10\n",
      "571/571 [==============================] - 7s 12ms/sample - loss: 0.0991 - main_output_loss: 0.0759 - aux_output_loss: 0.0231 - val_loss: 0.1102 - val_main_output_loss: 0.0629 - val_aux_output_loss: 0.0474\n",
      "Epoch 8/10\n",
      "571/571 [==============================] - 7s 12ms/sample - loss: 0.0799 - main_output_loss: 0.0570 - aux_output_loss: 0.0230 - val_loss: 0.1104 - val_main_output_loss: 0.0570 - val_aux_output_loss: 0.0534\n",
      "Epoch 9/10\n",
      "571/571 [==============================] - 7s 12ms/sample - loss: 0.0705 - main_output_loss: 0.0465 - aux_output_loss: 0.0241 - val_loss: 0.1037 - val_main_output_loss: 0.0621 - val_aux_output_loss: 0.0416\n",
      "Epoch 10/10\n",
      "571/571 [==============================] - 7s 12ms/sample - loss: 0.0904 - main_output_loss: 0.0657 - aux_output_loss: 0.0246 - val_loss: 0.1045 - val_main_output_loss: 0.0605 - val_aux_output_loss: 0.0440\n"
     ]
    }
   ],
   "source": [
    "# temp = pd.merge(inputs, outputs, on=[\"essay_id\", \"essay_id\"])\n",
    "# train=temp.sample(frac=1,random_state=200)\n",
    "# test=temp.drop(train.index)\n",
    "# main_input = np.array(train['pad_seq'].values.tolist())\n",
    "# aux_input = np.array(train[['sent_count','avg_sent_len','punc_count','oov_count','noun_count','verb_count','adv_count','adj_count','det_count','max_depth','avg_depth']].values.tolist())\n",
    "# main_output = np.array(train['score'].values.tolist())\n",
    "# model = build_model()\n",
    "# model.summary()\n",
    "# history = model.fit([main_input, aux_input], [main_output, main_output], validation_split = 0.1, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing model for score\n",
      "64/64 [==============================] - 1s 23ms/sample - loss: 0.2918 - main_output_loss: 0.1774 - aux_output_loss: 0.1144\n",
      "Prepared model for score\n",
      "Preparing model for Prompt Adherence\n",
      "35/35 [==============================] - 1s 34ms/sample - loss: 0.3055 - main_output_loss: 0.2653 - aux_output_loss: 0.0790\n",
      "Prepared model for Prompt Adherence\n",
      "Preparing model for Narrativity\n",
      "35/35 [==============================] - 1s 34ms/sample - loss: 0.4209 - main_output_loss: 0.1126 - aux_output_loss: 0.1748\n",
      "Prepared model for Narrativity\n",
      "Preparing model for Language\n",
      "42/42 [==============================] - 1s 31ms/sample - loss: 0.3429 - main_output_loss: 0.1674 - aux_output_loss: 0.1473\n",
      "Prepared model for Language\n",
      "Preparing model for Word Choice\n",
      "22/22 [==============================] - 1s 49ms/sample - loss: 0.2224 - main_output_loss: 0.1555 - aux_output_loss: 0.0669\n",
      "Prepared model for Word Choice\n",
      "Preparing model for Sentence Fluency\n",
      "22/22 [==============================] - 1s 50ms/sample - loss: 0.2909 - main_output_loss: 0.1468 - aux_output_loss: 0.1442\n",
      "Prepared model for Sentence Fluency\n",
      "Preparing model for Organization\n",
      "28/28 [==============================] - 1s 44ms/sample - loss: 0.2795 - main_output_loss: 0.1339 - aux_output_loss: 0.1456\n",
      "Prepared model for Organization\n",
      "Preparing model for Conventions\n",
      "28/28 [==============================] - 1s 43ms/sample - loss: 0.2019 - main_output_loss: 0.1782 - aux_output_loss: 0.0237\n",
      "Prepared model for Conventions\n",
      "Preparing model for Content\n",
      "64/64 [==============================] - 2s 26ms/sample - loss: 0.3807 - main_output_loss: 0.2463 - aux_output_loss: 0.1344\n",
      "Prepared model for Content\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': {'loss': [0.2918456792831421],\n",
       "  'main_output_loss': [0.17740113],\n",
       "  'aux_output_loss': [0.11444456]},\n",
       " 'Prompt Adherence': {'loss': [0.30553313578878133],\n",
       "  'main_output_loss': [0.26527154],\n",
       "  'aux_output_loss': [0.07904084]},\n",
       " 'Narrativity': {'loss': [0.4208550406353814],\n",
       "  'main_output_loss': [0.112552084],\n",
       "  'aux_output_loss': [0.17475058]},\n",
       " 'Language': {'loss': [0.3428544004758199],\n",
       "  'main_output_loss': [0.16735834],\n",
       "  'aux_output_loss': [0.14732656]},\n",
       " 'Word Choice': {'loss': [0.2224113643169403],\n",
       "  'main_output_loss': [0.15554683],\n",
       "  'aux_output_loss': [0.06686453]},\n",
       " 'Sentence Fluency': {'loss': [0.2909168601036072],\n",
       "  'main_output_loss': [0.14676675],\n",
       "  'aux_output_loss': [0.14415012]},\n",
       " 'Organization': {'loss': [0.2795073986053467],\n",
       "  'main_output_loss': [0.13389352],\n",
       "  'aux_output_loss': [0.1456139]},\n",
       " 'Conventions': {'loss': [0.20194461941719055],\n",
       "  'main_output_loss': [0.17819956],\n",
       "  'aux_output_loss': [0.023745058]},\n",
       " 'Content': {'loss': [0.38069483637809753],\n",
       "  'main_output_loss': [0.24626547],\n",
       "  'aux_output_loss': [0.13442937]}}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "required = ['score', 'Prompt Adherence','Narrativity', 'Language', 'Word Choice', 'Sentence Fluency', 'Organization', 'Conventions', 'Content']  \n",
    "histories = {}\n",
    "for desired in required:\n",
    "    print(\"Preparing model for \"+ desired)\n",
    "    temp = outputs[outputs[desired]==outputs[desired]]\n",
    "    temp = pd.merge(inputs, temp, on=[\"essay_id\", \"essay_id\"])\n",
    "    train=temp.sample(frac=1,random_state=200)\n",
    "    test=temp.drop(train.index)\n",
    "    main_input = np.array(train['pad_seq'].values.tolist())\n",
    "    aux_input = np.array(train[['sent_count','avg_sent_len','punc_count','oov_count','noun_count','verb_count','adv_count','adj_count','det_count','max_depth','avg_depth']].values.tolist())\n",
    "    main_output = np.array(train['score'].values.tolist())\n",
    "    model = build_model()\n",
    "#     model.summary()\n",
    "    histories[desired] = model.fit([main_input, aux_input], [main_output, main_output], validation_split = 0.1, epochs=10, verbose=1)\n",
    "#     histories[desired] = model.fit([main_input, aux_input], [main_output, main_output], epochs=1, verbose=1)\n",
    "    histories[desired] = histories[desired].history\n",
    "    print(\"Prepared model for \"+ desired)\n",
    "    model.save(\"./finalModels/EssayModel_\"+desired+\".h5\")\n",
    "with open('modelHistories.pkl', 'wb') as f:\n",
    "        pickle.dump(histories, f, pickle.HIGHEST_PROTOCOL)\n",
    "histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# temp = pd.merge(inputs, outputs, on=[\"essay_id\", \"essay_id\"])\n",
    "\n",
    "# trainX, trainY, testX, testY = buildData(np.array(temp['pad_seq'].values.tolist()),np.array(temp['score'].values.tolist()))\n",
    "# model = build_model()\n",
    "# model.summary()\n",
    "# ival = IntervalEvaluation(validation_data=(testX, testY*10), interval=1)\n",
    "# # history = model.fit(trainX, trainY, epochs=5,  callbacks=[ival], validation_split = 0.1, verbose=1)\n",
    "# history = model.fit(trainX, trainY, epochs=10,  callbacks=[ival], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.2918456792831421],\n",
       " 'main_output_loss': [0.17740113],\n",
       " 'aux_output_loss': [0.11444456]}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histories['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
